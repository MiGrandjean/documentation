////
; Copyright (c) uib gmbh (www.uib.de)
; This documentation is owned by uib
; Until we found a better license:
; All rights reserved.
; credits: http://www.opsi.org/credits/
////

:Author:    uib gmbh
:Email:     info@uib.de
:Date:      13.01.2012
:Revision:  4.0
:toclevels: 6


include::../common/opsi_terms.asciidoc[]

= opsi-Nagios-Connector

[[opsi-Nagios-Connector-introduction]]
== Einführung

Neben dem Client Management ist das Monitoring der IT-Infragstruktur eine der Kernfunktion in der heutigen IT. opsi ist ein Client-Management Werkzeug. Für das Monitoring gibt es erprobte und bekannte Opensource Lösungen. Daher der Ansatz opsi nicht um ein Monitoring zu erweitern, sondern eine Schnittstelle zu bestehenden Lösungen anzubieten. Mit dieser Erweiterung von opsi, wird eine Schnittstelle für die bekannte Monitoring Lösung Nagios zur Verfügung gestellt. In den folgenden Kapiteln wird der opsi-Nagios-Connector erläutert und Beispielkonfigurationen vorgestellt.

Der opsi-Nagios-Connector ist nicht fest an eine Monitoringsoftware gebunden. Programmiert wurde die Schnittstelle für Nagios/Icinga. Der Einsatz mit anderen Monitoring Lösungen ist denkbar, wird momentan aber weder getestet noch unterstützt.

Die folgende Dokumentation beschreibt wie der opsi-Nagios-Connector aufgebaut ist und wie man ihn konfigurieren muss. Es wird voraussgesetzt, dass eine funktionierende Installation von Nagios bzw. Icinga vorliegt.

[[opsi-Nagios-Connector-prerequires]]
== Vorbedingungen

Vorrausgesetzt wird eine Nagios Installation in einer aktuellen 3.x Version oder eine Icinga Installation mindestens in der Version 1.6. Der opsi-Nagios-Connector sollte auch mit allen Nagios-Kompatiblen Monitoring-Lösungen benutzbar sein, getestet wird diese opsi Erweiterung allerdings nur gegen Nagios und Icinga. Sollte eine grafische Darstellung der opsiconfd-Performance Werte gewünscht werden, wird zusätzlich noch eine Installation des Addons pnp4nagios benötigt.

Für weitere Informationen, siehe unter:

www.nagios.org

www.icinga.org

www.pnp4nagios.org

[[opsi-Nagios-Connector-concept]]
== Konzept

Der opsi-Nagios-Connector besteht haupstächlich aus zwei verschiedenen Komponenten. Im Folgenden werden beide Komponenten erläutert.

[[opsi-Nagios-Connector-concept-webservice]]
=== opsi-Webservice Erweiterung

Eines der Hauptschwerpunkte des opsi-Nagios-Connectors ist eine Erweiterung des opsi-Webservice. Die Webserviceschnittstelle bietet damit die Möglichkeit Checks vom opsi-server anzufordern. Das bedeutet der Nagios Server verwendet die erweiterten Webservicemethoden zum Auslösen und Abfragen von Checks. Der Vorteil dieser Lösung ist, dass die eigentliche Checkausführung im opsi-System ausgeführt wird und somit der Aufwand auf den überwachten Systemen gering bleibt.

Diese Erweiterung stellt eine Sammlung von Checks zur Verfügung, die in einem späteren Kapitel einzeln vorgestellt werden. Hauptsächlich wurden opsi spezifische Checks eingebaut. "Normale" Systemchecks werden nicht über den opsi-Nagios-Connector angeboten und müssen auf konventionelle Weise ausgeführt werden.

[[opsi-Nagios-Connector-concept-opsiclientd]]
=== opsi-client-agent Erweiterung

Ein weiterer Teil des opsi-Nagios-Connectors ist eine Erweiterung des opsi-client-agent. Da in einer opsi-Umgebung jeder Managed-Client einen laufenden opsi-client-agent hat, bietet sich an, diesen als Nagios-Agent zu benutzen. Wichtig bei dieser Erweiterung ist zu Wissen, dass nicht alle Funktionen eines Nagios-Agenten, wie z.B. `NSClient++` implementiert wurden.

Die Möglichkeiten des opsi-client-agent umfassen das Ausführen von plugin Befehlen auf der Kommandozeile und das Zurückliefern der Ergebnisse.
////
Wenn man nicht alle Funktionen, wie NSCA benötigt, sondern nur ein paar Standard-Checks per Plugin auf den Clients ausführen oder eine Reihe von eigenen Plugins auf den Clients benutzen möchte, kann man den opsi-client-agent dazu verwenden. 
////
Für Situationen in denen erweiterte Funktionen vom Nagios-Agent wie NSCA benötigt werden, wird ein opsi-Paket für die Verteilung und Pflege des `NSClient++` per opsi bereitgestellt.

Der Vorteil der Verwendung des opsi-client-agent ist zum einen, dass kein zusätzlicher Agent benötigt wird und das der Monitoring-Server keine Zugangsdaten vom Client haben muss, da die Checks zum Client alle über den opsiconfd laufen. Dies vereinfacht auch die Konfiguration auf der Monitoring Seite um einiges.

[[opsi-Nagios-Connector-checks]]
== opsi-Checks

Im folgenden Kapitel werden Zweck und Konfiguration der einzelnen opsi-Checks erklärt. 

[[opsi-Nagios-Connector-checks-background]]
=== Hintergrund zum richtigen Verteilen der Checks

In der allgemeinen Monitoring Sprache werden zwischen aktiven und passiven Checks gesprochen. Die Bedeutung besagt, ob Nagios/Icinga die Checks selber auslöst und ausführt (aktiv) oder ob die Hosts die Checks eigenständig ausführen und die Ergebnisse an Nagios/Icinga selbstständig zurückmelden (passiv).

Auch bei der opsi-Nagios-Connector-Erweiterung gibt es diese zwei Unterscheidungen. Allerdings werden diese bei opsi als direkte- bzw. indirekte-Checks bezeichnet:

* Direkt: Diese Checks werden auf dem Client ausgeführt und liefern Ergebnisse vom Client an den Monitoring Server.
* Indirekt: Die Erhebung der Daten, die für diese Art von Checks relevant sind, findet auf Basis von Backend-Daten im opsi-System statt. Diese können von realen Situationen und Zuständen abweichen.

Ein gutes Beispiel für einen inderekten Check ist `check-opsi-client-status`. Dieser Check bezieht sich eigentlich auf einen Client, ausgeführt wird er aber auf dem opsi-Backend. Der Client-Status ist in diesem Fall der Zustand des Clients aus Sicht von opsi. Im Gegensatz dazu wird jeder Check der tatsächlich am Client ausgeführt wird als direkter Check bezeichnet.

Die richtige Verteilung der Checks und damit die richtige Konfiguration erfordert zunächst die Analyse der eigenen opsi-Umgebung. Da opsi sehr flexibel einsetzbar ist, können verschiedene Szenarien auftreten. Hier werden die am häufigsten mit opsi eingesetzten Installationsvarianten abgehandelt. (Für spezielle Installation sollten Sie uns kontaktieren oder direkt über einen bestehenden Supportvertrag Ihre Situation schildern.)

Ein opsi-Server (Standalone Installation): Diese Variante ist die am häufigsten eingesetzten Variante. Bei dieser Installation ist der Configserver auch gleichzeitig der Depotserver und somit werden alle Checks über den opsi-configserver aufgerufen.

Mehrere opsi-Server mit einer zentralen Administration (Multi-Depot Umgebung): Um bei dieser Art der Installation die Checks richtig zu verteilen muss man als erstes Verstehen, wie eine Multidepot-Umgebung aufgebaut ist:

MULTIDEPOTBILD

Wie im Bild zu erkennen, gibt es nur ein Daten-Backend, welches am opsi-Configserver angesiedelt ist. Jeder Check der gegen das Backend ausgeführt wird muss somit zwangsläufig über den opsi-Configserver. Somit werden alle Checks die an die Depotserver gehen, intern an den Configserver weitergeleitet. Deshalb ist es sinnvoller diese Checks direkt gegen den opsi-Configserver aus zu führen. Eine Ausnahme können die aktiven Checks gegen den opsi-client-agent bilden. Wenn zum Beispiel zwischen den Servern eine Firewall aufgestellt ist, die nur den Port 4447 durchlässt, können Clients an der Aussenstelle eventuell nicht erreicht werden (Standardport 4441). In solchen Fällen kann es nützlich sein den aktiven Check am Depotserver in der Aussenstelle auszuführen.

VERTEILTE Checks Bild

[[opsi-Nagios-Connector-checks-plugin]]
=== opsi-check-plugin

Auf dem Nagios Server gibt es nur ein opsi-check-plugin, welches aber eine große Zahl von Checks unterstützt. Deshalb hat dieses Plugin auch ziemlich viele Optionen. Eine Auflistung dieser Optionen wäre der Erläuterung nicht dienlich, deshalb wird an dieser Stelle darauf verzichtet. Die einzelnen Optionen, die benötigt werden oder möglich sind, werden bei den einzelnen Checks erläutert. Das opsi-check-plugin wird aufgerufen mit dem Befehl `check_opsi`. Eine Übersicht der möglichen Optionen erhält man mit dem Parameter -h oder --help. +
Die folgenden Optionen sind für alle Checks notwendig:

.Allgemeine Optionen
|=======================
|Optionen|Bezeichnung|Beispiel
|-H,--host|opsiServer auf dem gecheckt werden soll|configserver.domain.local
|-P,--port|opsi-Webservice Port|4447 (Default)
|-u,--username|opsi Monitoring User|monitoring
|-p,--password|opsi Monitoring Password|monitoring123
|-t,--task|opsi Checkmethode (Case Sensitive)|
|=======================

Die oben aufgeführten Paramater müssen immer gesetzt werden. Das nachfolgende Kapitel erläutert als erstes, wie man das opsi-check-plugin manuell aufrufen würde. Wie diese über Nagios/Icinga gesetzt werden, wird im Kapitel der Konfiguration erläutert.

Um das check-plugin vom opsi-Nagios-Connector zu installieren, können Sie einfach das opsi-Repository auf dem Nagios-Server eintragen und mit folgendem Befehl das Paket installieren:

[source,prompt]
----
apt-get install opsi-nagios-plugins
----

Momentan gibt es Check-Plugin nur als Debian/Ubuntu-Pakete. Weitere Distributionen sind in Planung. Sollten das Check-Plugin für eine andere Distribution als Debian/Ubuntu benötigen, bitte kontaktieren Sie: info@uib.de. Das Plugin selbst ist in Python geschrieben und sollte auch auf anderen Distributionen laufen. Dieses Paket basiert auf dem Paket: 'nagios-plugins-basic' und installiert entsprechend das Plugin ins Verzeichnis: `/usr/lib/nagios/plugins`. Die Konfiguration der Nagios-Check-Commands wird nicht automatisch angelegt, da dieses Plugin sehr flexibel einsetzbar ist. Deshalb wird dieser Teil im Kapitel über die Konfiguration etwas später näher erläutert. 

[[opsi-Nagios-Connector-checks-opsiWebservice]]
==== Check: opsi-Webservice 

Mit diesem Check wird der opsi-Webservice-Prozess überwacht. Dieser Check liefert auch Performancewerte. Deshalb sollte dieser Check auf jedem opsi-Server selbst ausgeführt werden, da jeder Opsi-Server seinen eigenen opsiconfd-Prozess hat, der überwacht werden kann bzw. sollte.

[source,prompt]
----
check_opsi.py -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiWebservice
----

Dieser Check liefert in der Regel OK zurück. In folgenden Situationen kann dies Abweichen:

* Critical: Wenn der Webservice ein Problem hat und nicht richtig antwortet. Weiterhin wird dieser Status zurückgemeldet, wenn die CPU-Last des Prozesses 80% überschreitet oder wenn die prozentuale Wert von RPC-Errors 20% erreicht und übersteigt. (In Bezug auf die gesamten RPCs)
* Warning: Wenn der Webservice zwar arbeitet, aber Grenzwerte überschreitet: CPU Auslastung höher als 60% aber unter 80% oder wenn der prozentuale Wert der RPC-Errors in Bezug auf die Gesamt-RPC Anzahl höher als 10% aber unter 20% liegt.
* Unknown: Wenn der Webservice gar nicht erreichbar ist.

Info: Die CPU-Werte beziehen sich immer nur auf eine CPU, da ein Prozess auch nur einen Prozessor benutzen kann. Eine Ausnahme bildet hier das Multiprocessing von opsi.

[[opsi-Nagios-Connector-checks-opsiWebservice-pnp4nagios-template]]
===== Check: opsi-Webservice pnp4nagios-Template

Für die Perfomance-Auswertung gibt es ein Template für das pnp4nagios, welches die Werte kombiniert darstellt. Wie man das pnp4nagios installiert, wird hier nicht explizit beschrieben, sondern es wird davon ausgegangen, dass pnp4nagios richtig installiert und konfiguriert wurde. Die notwendige Vorgehensweise kann sich von der hier beschriebenen Lösung unterscheiden, wenn das pnp4nagios mit anderen Pfaden installiert wurde (kann bei selbst kompilierten Installationen vorkommen).

Es werden Standard-Templates verwendet, welche für jeden einzelnen Performancewert in eigenes Diagramm darstellen. Um das oben genannte Template mit der kombinierten Ansicht zu verwenden muss man folgendermaßen vorgehen:

Schritt 1: Erstellen Sie unterhalb von: `/etc/pnp4nagios/check_commands` eine Datei mit der Bezeichnung: `check_opsiwebservice.cfg` und folgendem Inhalt:

[source,cmd]
----
CUSTOM_TEMPLATE = 0
DATATYPE = ABSOLUTE,ABSOLUTE,ABSOLUTE,ABSOLUTE,DERIVE,GAUGE,GAUGE,GAUGE
----


Schritt 2: Legen Sie die Datei `check_opsiwebservice.php` unterhalb von `/usr/share/pnp4nagios/html/templates` ab. Diese Datei können Sie über svn.opsi.org auschecken.

[source,cmd]
----
cd /usr/share/pnp4nagios/html/templates
svn co https://svn.opsi.org/opsi-pnp4nagios-template/trunk/check_opsiwebservice.php
----

Wichtig bei diesen Templates ist, dass die Namen der PHP-Dateien mit genauso heißen wie der 'command_name' welcher in der Datei `/etc/nagios3/conf.d/opsi/opsicommands.cfg` definiert ist. Stimmen die Bezeichnungen nicht überein, wird ein Standardtemplate von pnp4nagios verwendet.

Sollte diese Anpassung vorgenommen werden, wenn die ersten Checks für den opsi-webservice schon stattgefunden haben, müssen die schon erstellten RRD-Datenbanken erst mal gelöscht werden, da mit diesen Templates auch die Struktur der RRD-Datenbanken neu konfiguriert werden.

In der Regel sind die RRD-Datenbanken unter folgendem Pfad zu finden: `/var/pnp4nagios/perfdata/<host>/`

Dabei reicht es aus alle Dateien die entweder mit opsi-webservice.rrd oder mit opsi-webservice.xml beginnen zu löschen. (Vorsicht: Hier werden auch andere RRD-Datenbanken von anderen Checks für diesen Host angelegt, die nicht unbedingt gelöscht werden sollten.)

Damit das ganze automatisch funktioniert, müssen diese Dateien genauso heißen, wie das check_command vom opsi-Webservice. Der Grund dafür liegt in der Arbeitsweise von pnp4nagios. Sollte also die Konfiguration des opsi-Webservice von dieser Dokumentation abweichen, so müssen auch diese template-Dateien umbenannt werden, da ansonsten pnp4nagios keine richtige Zuordnung treffen kann.

Sollte bis hierhin alles richtig konfiguriert worden sein und die Konfigurations-Schritte im Konfigurationskapitel richtig befolgt worden sein, sollten die Diagramme wie im folgenden Screenshot automatisch generiert werden:

image::../images/pnp4nagios.png["uib-Template für pnp4nagios",width=400]

[[opsi-Nagios-Connector-checks-opsidiskusage]]
==== Check: opsi-check-diskusage

Mit diesem Check werden die opsiconfd-Ressourcen auf Füllstand überwacht. Die Folgende Tabelle zeigt, welche Ressourcen damit gemeint sind.

.opsi Ressourcen
|=======================
|Ressource|Pfad
|/|/usr/share/opsiconfd/static
|configed|/usr/lib/configed
|depot|/opt/pcbin/install
|repository|/var/lib/opsi/repository
|=======================

Bitte auch hier beachten, dass nur die Füllstände der Pfade, die für opsi Relevant sind, beim Check berücksichtigt werden. Dieser Check soll keinen allgemeinen DiskUsage-Check ersetzen.

Mit folgendem Befehl kann man alle Ressourcen gleichzeitig Abfragen:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDiskUsage
----

Zusätzlich zu der normalen Checkvariante gibt es die Möglichkeit nur eine Ressource zu checken. Das folgende Beispiel checkt nur nach der Resource `depot`:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDiskUsage -r depot
----

Standardmäßig gibt dieser Check OK zurück und gibt den freien Speicherplatz der Resource oder der Resourcen zurück. Die Einheit ist dabei Gigabyte. Folgende weitere Zustände sind möglich:

* WARNING: Wenn eine oder mehrere Resourcen 5GB oder weniger freien Speicherplatz haben.
* CRITICAL: Wenn eine oder mehrere Resourcen 1GB oder weniger freien Speicherplatz haben.

Die oben genannten Werte sind die Standard-Thresholds. Diese können durch zusätzliche Angabe von den Optionen -C und -W bzw. --critical und --warning selber bestimmt werden. Dabei gibt es zwei mögliche Einheiten: 10G bedeutet mindestens 10 Gigabyte freier Speicherplatz, durch diese Angabe wird der Output auch in dieser Datei ausgegeben. Wenn die Thresholds in prozent oder ohne Angaben angegeben werden, wird auch der Output in Prozent generiert. Abschliessend noch ein Beispiel zur Veranschaulichung:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDiskUsage -r depot --warning 10% --critical 5%
----

[[opsi-Nagios-Connector-checks-clientstatuscheck]]
==== Check: opsi-client-status 

Einer von zwei Schwerpunkten des opsi-Nagios-Connectors ist das Überwachen von Software-Rollouts. Dafür wurde dieser Check konzipiert. Er bezieht sich immer auf einen Client innerhalb von opsi.

Der Status der Produkte auf dem Client entscheidet über das Ergebnis des checks. Auf diese Weise erkennt man schnell, ob eine Produktinstallation auf dem betreffenden Client ansteht oder aktuell ein Problem verursacht hat. Aber nicht der Produktstatus des Clients kann das Checkergebnis beeinflussen, sondern auch wann dieser Client sich das letzte mal am service gemeldet hat. Wenn der Client länger als 30 Tage nicht am Service war, wird der Check mindestens ein Warning als Ergebnis zurückgeben. Dieser Zeitraum orientiert sich an der Abfolge der Microsoft Patchdays. Wenn ein Client länger als 30 Tagen von der Softwareverteilung abgeschottet ist, sollte man den Client checken. Auf diese Weise kann man auch Clients im System finden, die schon lange inaktiv sind, die man im normalen Betrieb schnell übersieht.

Dieser Check kann auf verschiedene Weisen durchgeführt werden, die einfachste Variante:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkClientStatus -c opsiclient.domain.local
----

Man kann einzelne Produkte anhand Ihre ID von diesem Check ausschliessen. Dies würde zum Beispiel so aussehen:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkClientStatus -c opsiclient.domain.local -x firefox
----

Das obige Beispiel schliesst das Produkt firefox aus diesem Check aus, somit würde dieser Check auch ein OK liefern, selbst wenn das Produkt firefox bei dem Client auf failed steht.

[[opsi-Nagios-Connector-checks-opsiproductstatus]]
==== Check: opsi-check-ProductStatus

Der zweite Schwerpunkt des opsi-Nagios-Connectors ist das Überwachung von Software-Rollouts. Mit die wichtigste Aufgabe in einem Software-Verteilungssystem. Sobald eine neue Software, egal ob Standard-Software, oder eigene Software mit opsi Verwaltet, Verteilt und aktuell gehalten wird, muss man den Rollout-Status im Auge behalten.

Durch einige Parameter kann man diesen Check flexibel einsetzen. Um dies besser zu verdeutlichen werden einige Beispielaufrufe aufgeführt und erläutert. Im einfachsten Fall ruft man den Check einfach für ein Produkt auf. Dieser wird als `productId` (opsi-Interner Name) angegeben.

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox
----

In einer normalen opsi-Umgebung reicht dieser Aufruf, um den Zustand für das Produkt `firefox` zu überwachen. Die Ausgabe zeigt an, ob alles in Ordnung ist oder wieviele Installationen anstehen (setup), wieviele Clients ein Problem mit diesem Produkt haben (failed) und wieviele Clients nicht die aktuelle Version installiert haben.

Dies ist zur Übersicht schon in den meisten Fällen ausreichend, wenn man aber nun genau wissen will, auf welchen Clients, welcher Zustand zu diesem Produkt zu diesem Checkergebnis geführt hat kann man den Befehl im `verbosemode` ausführen:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox -v
----

Bei einer Multidepot-Umgebung wird der obige Befehl aber nicht die komplette Umgebung nach diesem Produkt überwachen, sondern nur den Configserver.  (Oder exacter: Die clients die dem depot auf dem config server zugewiesen sind). Wenn man mehrer Depotserver hat, kann man das Depot auch direkt mit angeben:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox -d depotserver.domain.local
----

Der Grund dafür ist, dass jeder Depotserver diese Produkt zur Verteilung haben kann. Dies muss nicht überall die selbe Version sein, auch wenn die productId genau die selbe ist. Aus diesem Grund müssen alle Clients anders beurteilt werden, je nachdem an welchem Depot sie registriert sind. Dies hat zusätzlich noch den Vorteil, dass man diesen Check später im Nagios beim Depotserver ansiedeln kann, was zusätzlich die Übersichtlichkeit erhöht. Wenn man nur ein oder zwei Depotserver hat, kann man auch mit der Angabe von `all` alle Depotserver mit einem Check abdecken oder Kommasepariert mehrere Depotserver angeben.

Zusätzlich kann man mit diesem Check auch mit opsi-Gruppen arbeiten. Man kann zum Beispiel eine ganze Produktgruppe mit einem Check abfragen. Wenn man zum Beispiel eine Produktgruppe: `buchhaltung` bildet, kann man mit folgendem Aufruf:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -g buchhaltung
----

Nun werden alle Produkte, die Mitglieder in dieser Produktgruppe sind über diesen Check überwacht. Die Auswertung dieser Gruppe findet immer während des eigentlichen Checks statt, das bedeutet, man kann über opsi diese Gruppe bearbeiten und beeinflusst somit direkt die Check-Parameter ohne das man die Nagios-Konfiguration anpassen muss. (HINWEIS: Gruppen innerhalb von Gruppen werden nicht beachtet und müssen separat angegeben werden.)

Auch die Clients die für diesen Check abgearbeitet werden können beeinflusst werden. Wie vorher schon erwähnt, wird die Clientliste durch Angabe des Depotservers beeinflusst, zusätzlich können opsi-Hostgruppen angegeben werden, die für diesen Check abgearbeitet werden. Dieser Aufruf sieht zum Beispiel wie folgt aus:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -g buchhaltung -G produktivclients
----

Dies würde die Produkte für alle Clients der Gruppe `produktivclients` überprüfen. Auch bei den Hostgruppen gilt die Regel, dass Untegruppen dieser Gruppe nicht abgearbeitet werden. Für opsi-Productgroups gilt genauso, wie für opsi-Hostgroups, dass mehrere Gruppen Kommasepariert angegeben werden können.

Abschliessend kann man auch bei diesem Check opsi-Clients ausschliessen:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -x client.domain.local
----



[[opsi-Nagios-Connector-checks-opsidepotsync]]
==== Check: opsi-check-Depotsync 

Gerade in einer Mutlidepotumgebung ist es wichtig, die Depotserver auf Synchronität zu überwachen. Entscheidend ist bei diesem Check die Software- und Paketversion der installierten Produkte. Manchmal ist eine differentierter Einsatz der opsi-Produkte auf Depotservern gewünscht, birgt aber die Gefahr, dass bei einem Umzug von einem Client, von einem Depot zum anderen, inkonsistenzen in der Datenbank enstehen können. Um dieser Problematik entgegen zu wirken, wird empfohlen die opsi-Pakete auf den Depotservern so synchron wie möglich zu halten.

Standardmäßig liefert dieser Check OK zurück, sollte eine Differenz festgestellt werden, wird der Status: WARNING zurückgegeben. Dieser Check ist ein klassischer Check, der auf dem Configserver ausgeführt werden sollte, da alle Informationen zu diesem Check nur im Backend auf dem opsi-Configserver zu finden sind.

Als nächstes folgen ein paar Anwendungsmöglichkeiten dieses Checks:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus
----

Dies ist dies Basis-Variante und äquivalent zu folgendem Aufruf:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d all
----

Ohne konkrete Angabe von Depotserver werden die Produkt-Listen aller Depot-Server miteinander verglichen. Um eine bessere Übersichtlichkeit zu schaffen, sollte man diesen Check auf zwei Depotserver reduzieren und lieber auf mehrere Checks verteilen. Dies erreicht man durch direkte Angabe der Depotserver:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local
----

Mit diesem Aufruf werden alle Produkte verglichen, die auf beiden Depotservern installiert sind. Sollte ein Produkt auf einem Depotserver gar nicht installiert sein, hat dies keine Auswerkungen auf das Check-Resultat. DIes kann man ändern, indem man diesem Check den "strictmode" Schalter setzt:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local --strictmode
----

Nun werden auch Produkte angezeigt, die auf einem Depotserver nicht installiert sind. Um ein bestimmtes Produkt oder bestimmte Produkte nicht mit zu checken, weil man zum Beispiel will dass diese Produkte in verschiedenen Versionen eingesetzt werden, kann man diese Produkte von diesem Check ausschliessen:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local --strictmode -x firefox,thunderbird
----

Dieser Check würde auch dann ein OK zurückgeben, wenn das firefox und das thunderbird-Paket nicht überall Synchron eingesetzt werden.

Ein weitere Einsatzmöglichkeit ist, dass nur eine Auswahl von Produkten auf Synchronität überwacht werden können. Dies kann man durch:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local --strictmode -e firefox,thunderbird
----

So werden nur firefox und thunderbird Produkte auf Synchronität überwacht. Bei diesem Check sollte der `strictmode` gesetzt sein, damit man auch erkennt, wenn die gewünschten Produkte auf Depotservern nicht installiert sind.


[source,prompt]
----
./check_opsi.py -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSync
----

[[opsi-Nagios-Connector-checks-pluginOnClient]]
==== Check: Pugin über OpsiClientd checken

Dieser Check führt ein Check-Plugin auf dem Client direkt aus und fängt die Ausgabe ein. 

Diese Erweiterung soll keinen Ersatz für einen richtigen Nagiosagent bieten, sonderne eine alternative bieten. Man kann diese Erweiterung Einsetzen, wenn man Plugins auf dem opsi-Client checken will. Die eingesetzten Plugins müssen der sogenannten: `Nagios plug-in development guidelines` entsprechen. (Weitere Infos unter: http://nagiosplug.sourceforge.net/developer-guidelines.html)

Um ein Plugin ausführen zu können, muss man das Plugin erst einmal auf den Clients verteilen. Dies sollte man über ein opsi-Paket lösen. Der Ablageort für die Plugins auf dem Client ist im ersten momentan egal, da man den Pfad beim Checken mit angeben muss. Allerdings sollte man die Plugins nicht einzeln verteilen, sondern in einem Verzeichnis zusammenführen, damit das Aktualisieren und Pflegen der Plugins einfacher wird. Weiterhin sollte man auch Sicherheitstechnisch im Hinterkopf behalten, dass die Plugins im Systemkontext des opsiclientd-Services aufgerufen werden. Normale Anwender sollten auf dieses Verzeichnis keinen Zugang haben.

Es gibt Diverse Plugins, die es schon vorgefertigt im Internet runter zu laden gibt. Eine mögliche Anlaufstelle ist (http://exchange.nagios.org/)

Im folgenden wird davon ausgegangen, dass unter C:\opsi.org\nagiosplugins\ das Plugin check_win_disk.exe vom Paket nagioscol (http://sourceforge.net/projects/nagiosplugincol/) abgelegt wurde.

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkPluginOnClient --plugin "C:\opsi.org\nagiosplugincol\check_win_disk.exe C:" -c client.domain.local
----

Dieser Aufruf checkt auf dem Client `client.domain.local` das Plugin check_win_disk.exe und übergibt diesem den Parameter `C:`. Dies bedeutet, dass das Laufwerk C auf dem Client gecheckt wird. Die Ausgabe und der Rückgabewert dieses Plugins, wird direkt richtig ausgewertet und für Nagios verarbeitbar weitergereicht.

Ein besonderes Feature ist das beibehalten von Zuständen. Diese Implementation ist aus der Problemstellung entstanden, dass Clients nicht wie Server durchlaufen, sondern in der Regel nur einen bestimmten Zeitraum eingeschaltet sind. Man kann den Check auf Nagios-Seite zwar mit sogenannten `Timeperiods` eingrenzen, aber in der Praxis ist so ein vorgehen nicht praktikabel, da man zum Beispiel auch bei Urlaub von Anwendern flexibel reagieren müsste. Dies würde eine ständige Konfigurationsarbeit nach sich ziehen. Wenn man darauf verzichtet, wird der Status ständig geändert, auch wenn ein aufgetretenes Problem noch gar nicht gelöst ist. Deshalb kann man den letzten bekannten Status an opsi Übergeben. Sollte der Client nicht erreichbar sein, wird dieser letzte bekannte Status zurückgegeben. Ein Critical-Zustand zum Beispiel, bleibt in diesem Falle auch auf Critical stehen und wechselt nicht auf Unknown, was rein logisch aber richtig wäre.

Um dieses Feature später mit Nagios zu verwenden kann man die Nagios-Makros: $SERVICESTATEID$ und $SERVICEOUTPUT$ verwenden.

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkPluginOnClient --plugin "C:\opsi.org\nagiosplugincol\check_win_disk.exe C:" -c client.domain.local -s $SERVICESTATEID$ -o  $SERVICEOUTPUT$
----

[[opsi-Nagios-Connector-configuration]]
== opsi Monitoring Konfiguration

Dieses Kapitel widmet sich der Konfiguration der Schnittstelle von opsi und dem Nagios-Server. Die Konfigurationen in diesem Kapitel, besonders die auf dem Nagios-Server sollen als Empfehlungen gelten sind aber nicht die einzigen Lösungen. 
Hier wird nur die Konfiguration mit einem Nagios-Server beschrieben. Mit einem Icinga-Server sollte, mit Ausnahme von ein paar Pfaden, die Konfiguration ziemlich genauso funktionieren. Andere Derivate auf Nagios Basis sollten funktionieren, wurden aber nicht getestet.

[[opsi-Nagios-Connector-configuration-User]]
=== opsi Monitoring User

In der Regel wird im Monitoring-Bereich viel mit IP-Freischaltungen als Sicherheit gearbeitet. Da aber dieser Mechanismus nicht wirklich einen Schutz bietet, wurde beim opsi-Nagios-Connector darauf verzichtet. Aus diesem Grund wird das ganze per Benutzer und Passwort geschützt. Diesen User als opsi-admin einzurichten, würde aber auch hier zu viele Rechte freischalten, da dieser User nur für diese Schnittstelle von nöten ist und auch die Benutzbarkeit auf diesen Bereich eingeschränkt werden soll, wird der User nur intern in opsi eingerichtet. Folgender Befehl legt den User an:

[source, prompt]
----
opsi-admin -d method user_setCredentials monitoring monitoring123
----

Dieser Befehl legt den User: monitoring mit dem Passwort monitoring123 an. Der User wird in der /etc/opsi/passwd angelegt und ist auch kein User, mit dem man sich an der Shell anmelden könnte. 

Bei einer Multidepot-Umgebung muss man diesen User nur auf dem Configserver ereugen.

Beim nagios-Server kann man dieses Passwort vor den CGI-Skripten maskieren, indem man einen Eintrag in der /etc/nagios3/ressource.cfg vornimmt. Dieser sieht zum Beispiel so aus:

[source, prompt]
----
$USER2$=monitoring
$USER3$=monitoring123
----


Die Zahl hinter USER kann varieren. Wenn diese Datei vorher nicht genutzt wurde, sollte in der Regel nur das $USER1$ belegt sein. Diese Konfiguration dient als Grundlage für die weiteren Konfigurationen.

[[opsi-Nagios-Connector-configuration-template]]
=== Nagios Templates

Es gibt diverse Templates für diverse Objekte im Nagios. Dies ist eine Standard-Nagios Funktionalität, die hier nicht näher beschrieben wird. Diese Funktionalität kann man sich für opsi zu nutze machen, um sich später bei der Konfiguration die Arbeit zu vereinfachen.

Da die meisten Checks auf den Configserver ausgeführt werden, sollte man sich als erstes ein Template für die opsi-Server und ein Template für die opsi-Clients schreiben. Da es in einer Multidepot-Umgebung nur einen Configserver geben kann, kann man direkt diesen ins Template mit übernehmen. Dies erreicht man am einfachsten über eine Custom-Variable. Diese erkennt man daran, dass Sie mit einem `_` beginnen. Es wird in das Template für den opsi-Server und für die opsi-Clients folgendes zusätzliche in das Template eingetragen:

[source,prompt]
----
_configserver           configserver.domain.local
_configserverurl        4447
----

Auf diese beiden `custum Variablen` kann man später einfach mit dem Nagios-Makro: `$_HOSTCONFIGSERVER$` und `$_HOSTCONFIGSERVERPORT$`, verweisen. HOST muss vorangeführt werden, da diese Custum-Variablen in einer Hostdefinition vorgenommen wurden. Weitere Informationen zu Custom-Variablen entnehmen Sie bitte der Nagios-Dokumentation. Da diese beiden Konfigurationen im Template vorgenommen werden müssen, gelten Sie für jede Hostdefinition, die später von diesen Templates erben. Aber dazu später mehr.

Um nun die Templates an zu legen sollte man unterhalb von: `/etc/nagios3/conf.d` als erstes ein Verzeichnis `opsi` erstellen. Dies macht die Konfiguration später Übersichtlicher, da alle Konfigurationen die den opsi-Nagios-Connector betreffen gebündelt auf dem Server abgelegt sind. In diesem Verzeichnis erstellt man die Datei und benennt Sie auch direkt so, dass man später erkennt, was genau hier Konfiguriert werden soll: `opsitemplates.cfg`. Nun kann man diese Datei noch mit Inhalt füllen. Zunächst die Opsi-Server:

[source,ini]
----

----

die Standardmäßig geplant sind. Es gibt ein paar Templates, die dennoch erstellt werden sollten, diese werden hier kurz erläutert und sollen einen Einblick dazu geben, zur Besseren Verständlichkeit wurden die Kommentare zu den einzelnen Einstellungen nicht gelöscht.

Die folgenden Einstellungen sind in der Regel in der Datei /etc/nagios3/templates.cfg zu erledigen.

Zunächst sollte ein neues Template-Objekt für die Opsi-Clients erstellt werden:

[source, prompt]
----
#OPSI-Clients
define host{
        name                            opsiclient    ; The name of this host template
        notifications_enabled           1       ; Host notifications are enabled
        event_handler_enabled           1       ; Host event handler is enabled
        flap_detection_enabled          1       ; Flap detection is enabled
        failure_prediction_enabled      1       ; Failure prediction is enabled
        process_perf_data               0       ; Process performance data
        retain_status_information       1       ; Retain status information across program restarts
        retain_nonstatus_information    1       ; Retain non-status information across program restarts
                max_check_attempts              10
                notification_interval           0
                notification_period             24x7
                notification_options            d,u,r
                contact_groups                  admins
        register                        0       ; DONT REGISTER THIS DEFINITION - ITS NOT A REAL HOST, JUST A TEMPLATE!
        icon_image                      opsi/opsi-client.png
        }
----

Hauptsächlich sollte dieses Template erstellt werden aus folgenden Gründen:

* Da die Clients in der Regel nicht durchlaufen, sollte die Option: "check command	check-host-alive" nicht gesetzt wird. Somit werden die Clients als Pending angezeigt und nicht als Offline.
* Optional kann durch die Option icon_image ein Image gesetzt werden, dieser muss relativ zum Pfad: /usr/share/nagios3/htdocs/images/logos/ angegeben werden.
* Optional kann auch eine eigene contact_group angegeben werden, die allerdings als Contact-Object in der contacts.cfg angelegt sein muss.

Nach dem selben Prinzip kann auch ein Template für opsi-Server erstellt werden. Auf ein weiteres Template-Beispiel wird verzichtet, da die Templates sich sehr ähneln.

Wenn pnp4nagios für die grafische Darstellung von den Performancedaten für den Opsi-Webservice eingesetzt werden, sollten noch folgende zwei Objekte als Template angleget werden:

[source,prompt]
----
define host {
   name       host-pnp
   action_url /pnp4nagios/index.php/graph?host=$HOSTNAME$&srv=_HOST_
   register   0
}

define service {
   name       srv-pnp
   action_url /pnp4nagios/index.php/graph?host=$HOSTNAME$&srv=$SERVICEDESC$
   register   0
}
----

[[opsi-Nagios-Connector-configuration-hostobjects]]
=== opsi-Server und opsi-Clients

Um die Übersichtlichkeit zu wahren, sollte unterhalb von `/etc/nagios3/conf.d` ein Verzeichnis opsi erstellt werden. Somit bleiben die opsi-Infrastruktur betreffenden Konfigurationen an einer Stelle gekapselt.

Als nächstes sollten folgende Hostgruppen erstellt werden. Dies dient zum einen der besseren Übersicht auf der Weboberfläche und hiltf beim konfigurieren der Services. Dafür sollte eine Datei Namens opsihostgroups.cfg mit folgendem Inhalt erstellt werden:

[source,prompt]
----
define hostgroup {
        hostgroup_name  opsi-clients
        alias           OPSI-Clients
}

define hostgroup {
        hostgroup_name  opsi-server
        alias           OPSI-Server
}
----

Als nächstes sollten die opsi-Server konfiguriert werden. Dies kann jeweils in einer eigenen Datei wie zum Beispiel configserver.domain.local.cfg oder eine Datei mit allen opsi-Host wie opsihost.cfg. Der Inhalt sollte für opsiserver folgendermaßen aussehen:

[source,prompt]
----
define host{
        use				opsiserver
        host_name		configserver
        hostgroups		opsi-server
        alias				opsi Configserver
        address			configserver.domain.local
        }

define host{
        use				opsiserver
        host_name		depotserver
        hostgroups		opsi-server
        alias				opsi Depotserver
        address			depotserver.domain.local
        }
----

Folgende Erläuterungen zu den Werte:
* use bezeichnet welches Template benutzt wird.
* hostgroups gibt an, welcher Hostgruppe der Server angehört.

Die opsi-Clients sollten mindestens folgendermaßen definiert werden:

[source,prompt]
----
define host{
        use				opsiclient
        host_name		client.domain.local
        hostgroups		opsi-clients
        alias				opsi client
        address			client.domain.local
        _depot_id			configserver.domain.local
        }
----

Die Clientkonfiguration bedient sich einer Sonderfunktion von Nagios. Die Option `_depot_id`
ist eine sogenannte Benutzerdefinierte Variable, die als Makro `$_depot_id$` benutzt werden kann. Dies ist nötig da die meisten Checks über den Configserver ausgeführt werden.

[[opsi-Nagios-Connector-configuration-commands]]
=== opsi Check-Kommandos Konfiguration

Die oben beschriebenen Check-Kommandos müssen nun mit den bis bisherigen Kommandos konfiguriert werden. Dafür solle eine Datei mit dem Namen opsicheckcommands.cfg erstellt werden. Hier wird eine Bespielkonfiguration angegeben, je nach dem welche Funktionen Sie auf welche Weise verwenden wollen, müssen Sie diese Einstellungen nach eigenem Ermessen modizieren.

Als erstes wird der Aufbau Anhand eines Beispiels erläutert:

[source,prompt]
----
define command{
        command_name	check_opsi_clientstatus
        command_line		$USER1$/check_opsi.py -H $_HOSTDEPOT_ID$ -u $USER2$ -p $USER3$ -P 4447 -t checkClientStatus -c $HOSTADDRESS$
        }
----

Der `command_name` dient zur Referenzierung der weiteren Konfiguration. In der Option `command_line` werden die Konfigurationen  und das Check-Kommando als erstes vereint.

Nach diesem Prinzip baut man nun die ganze Datei auf:

[source,prompt]
----
define command{
        command_name    check_opsi_clientstatus
        command_line    $USER1$/check_opsi.py -H $_HOSTDEPOT_ID$ -p 4447 -t checkClientStatus -c $HOSTADDRESS$
        }
define command{
        command_name    check_opsi_productStatus
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -P $ARG1$ -t checkProductStatus
        }
define command{
        command_name    check_opsi_productStatus_detailed
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -P $ARG1$ -t checkProductStatus -v
        }
define command{
        command_name    check_opsi_productStatus_group
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -g $ARG1$ -t checkProductStatus -v -d $HOSTADDRESS$
        }
define command{
        command_name    check_opsi_pluginonclient
        command_line    $USER1$/check_opsi.py -H $_HOSTDEPOT_ID$ -p 4447 -t checkPluginOnClient -c $HOSTADDRESS$ --plugin $ARG1$
}
define command{
        command_name    check_opsi_pluginonclient_withState
        command_line    $USER1$/check_opsi.py -H $_HOSTDEPOT_ID$ -p 4447 -t checkPluginOnClient -c $HOSTADDRESS$ --plugin $ARG1$ -s $RG2$ -o $ARG3$
}

define command {
        command_name    check_opsiwebservice
        command_line    $USER1$/check_opsi_webservice.py -H $HOSTADDRESS
}

define command {
        command_name    check_opsi_depotsync
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -t checkDepotSyncStatus -d $ARG1$
}

define command {
        command_name    check_opsi_depotsync_withexclude
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -t checkDepotSyncStatus -d $ARG1$ -x $ARG2$
}
----


