////
; Copyright (c) uib gmbh (www.uib.de)
; Cette documentation appartient à uib
; et publié sous licence Creative Commons by-sa allemande
; voir:
; http://creativecommons.org/licenses/by-sa/3.0/de/
; http://creativecommons.org/licenses/by-sa/3.0/de/legalcode
; anglais:
; http://creativecommons.org/licenses/by-sa/3.0/
; http://creativecommons.org/licenses/by-sa/3.0/legalcode
; 
; crédits: http://www.opsi.org/credits/
////


:Author:    OpenSides sprl
:Email:     opsi@opensides.be
:Date:      31.05.2012
:Revision:  4.0
:toclevels: 6


include::../common/opsi_terms.asciidoc[]

[[opsi-Nagios-Connector]]
== Connecteur-opsi-Nagios

[[opsi-Nagios-Connector-introduction]]
=== Introduction

Outre la gestion des clients, la surveillance est l'une des fonctions centrales dans une gestion des services informatiques modernes. Avec OPSI vous avez un outil de gestion clients. Pour les tâches de surveillance il y a d'autres bien connus solutions open source. Alors nous construisons pour les tâches de surveillance dans OPSI non pas un outil de surveillance propre mais une interface aux solutions existantes. Avec cette extension OPSI nous fournissons un connecteur à Nagios. +
Dans les chapitres suivants est expliquée la conception et la configuration du connecteur opsi Nagios.

Le connecteur opsi Nagios n'est pas strictement liée à Nagios. Il est développé pour l'utilisation avec Nagios et Icinga. Il devrait également fonctionner avec des dérivés de Nagios mais cela n'est pas testé ni pris en charge.

La portée de ce manuel est la conception et la configuration du connecteur opsi-Nagios. Il n'est pas un manuel Nagios. Vous aurez besoin d'une installation de Nagios et de savoir comment faire fonctionner Nagios.

[[opsi-Nagios-Connector-prerequires]]
=== Conditions préalables

[[opsi-Nagios-Connector-prerequires-opsi]]
==== Conditions préalables au niveau du serveur et des clients OPSI

Ce module est développé en tant que projet en co-financement. Cela signifie qu'il n'est disponibles que pour les clients qui paient une contribution aux coûts de développement ou à des fins d'évaluation. Dès que le développement du projet sera refinancé, le composant fera partie de la distribution opsi et pourra être utilisé gratuitement.  +
voir aussi +
http://http://www.opsi.org/fr/projets-de-co-financement +
http://www.opsi.org/fr/statistics

Donc, comme condition sine qua non pour utiliser cette extension, vous avez besoin d'abord d'un fichier d'activation.
À des fins d'évaluation vous obtiendrez un fichier d'activation temporaire si vous le demandez dans un mail à opsi@opensides.be.

Les onditions préalables techniques sont opsi 4.0.1 avec le suivant paquets et versions de produits:

.Versions des produits et des paquets nécessaires
[options="header"]
|==========================
|opsi package|Version
|opsi-client-agent|>=4.0.1-26
|opsiconfd|>=4.0.1.11-1
|python-opsi|>=4.0.1.38-1
|==========================

[[opsi-Nagios-Connector-prerequires-nagios]]
==== Conditions préalables au niveau du serveur Nagios


Comme condition préalable, vous avez besoin d'une installation de Nagios dans la version 3.x ou d'une installation Icinga dans la version 1.6 ou supérieur. +
Pour une sortie graphique de données sur le rendement, l'installation de pnp4nagios est nécessaire.

Vous trouverez de plus amples informations à:

www.nagios.org

www.icinga.org

www.pnp4nagios.org

[[opsi-Nagios-Connector-concept]]
=== Concept

Le Connecteur opsi-Nagios contient deux composantes fondamentales. Dans un premier temps nous allons discuter du coeur du système.

[[opsi-Nagios-Connector-concept-webservice]]
==== extension opsi web service

Le coeur du connecteur opsi-Nagios sont des fonctions étendues du service Web OPSI. Ces extension de service Web permettent d'exécuter des vérifications via le web service sur le serveur opsi. Donc le serveur Nagios appelle des contrôles via le web service qui sont exécutés sur le serveur opsi et les résultats reviennent au serveur Nagios via le web service opsi. L'avantage de cette solution c'est qu'il n'y a presque rien à faire sur le serveur opsi surveillé.

L'objectif de l'extension de service Web OPSI se trouve sur des contrôles OPSI spécifiques comme par exemple le suivi de déploiement. Pour la surveillance 'normal' du serveur, vous devriez utiliser toujours des méthodes de contrôle standards. 

[[opsi-Nagios-Connector-concept-opsiclientd]]
==== extension opsi-client-agent

Une autre partie du Connecteur opsi-Nagios est l'extension de opsi-client-agent. +
Dans un environnement OPSI sur chaque client géré s'exécute opsi-client-agent. Avec cette extension, vous pouvez utiliser opsi-client-agent comme agent de Nagios ainsi. Mais en fait toutes les fonctions d'un agent standard Nagios ne sont pas mis en œuvre dans opsi-client-agent comme `NSClient++`. Vous pouvez utiliser opsi-client-agent pour exécuter des programmes en ligne de commande et renvoyer la sortie.

Si vous n'utilisez pas toutes les fonctions comme NSCA mais plutôt certains contrôles standard par plug-in sur le client ou un ensemble de plugins à vous sur les clients vous pouvez utiliser opsi-client-agent.

Si vous avez besoin de plus de fonctionnalités pour le suivi des clients vous devriez déploiement, par l'intermédiaire opsi, un agent standard comme `NSClient++`.

L'avantage d'utiliser opsi-client-agent comme agent de Nagios est que vous n'avez pas besoin d'un agent supplémentaire sur le client et que vous n'avez pas besoin de données d'accès pour les clients au niveau du serveur de surveillance. Ces données ne sont pas nécessaires parce que tout vérification sont exécuté via le serveur OPSI. Cela rend la configuration beaucoup plus facile.

[[opsi-Nagios-Connector-checks]]
=== opsi-checks

Le chapitre suivant explique les objectifs et les configurations des contrôles OPSI.

[[opsi-Nagios-Connector-checks-background]]
==== Quelques renseignements généraux sur l'endroit où exécuter les contrôles

Les administrateurs de surveillance connaissent la différence entre les contrôles actifs et passifs. 

Avec le Connecteur opsi-Nagios on obtient une nouvelle différence: directe et indirecte.

* directe: +
Le contrôle qui recueille des informations sur un client s'exécute sur ce client, il reçoit l'information directement auprès du client et renvoie l'information.

* indirecte: +
Le contrôle qui recueille des informations sur un client s'exécute sur le serveur opsi et il reçoit l'information à partir des données de configuration d'opsi qui sont stockées dans le backend d'opsi. Donc, cette information peut être différente de la situation réelle du client.

Un bon exemple pour un contrôle indirect est le `check-opsi-client-status`. Cette vérification vous donne, pour un client donné, les informations sur les demandes d'action en attente et les défaillances signalées du déploiement de logiciel d'opsi. Donc, ce sont des informations sur le client à partir du point de vue du serveur opsi. Par conséquent, cette vérification s'exécute sur le serveur OPSI et c'est un contrôle indirecte. Une vérification qui s'exécute sur le client est un contrôle direct.

Pour une correcte distribution et configuration des contrôles vous devez analyser votre installation OPSI. +
En fonction de la flexibilité de OPSI de nombreuses configurations différentes d'opsi sont possibles. Donc, ici nous ne pouvons qu'expliquer certaines situations typiques. Bien sûr, nous allons vous aider pour des situations particulières avec notre support commercial.

un seul serveur opsi: +
Une installation autonome d'opsi est la situation que vous trouverez dans la plupart des environnements OPSI. Dans ce type d'installation le serveur opsi ainsi que le serveur de depot opsi (un seul) sont sur la mëme machine. +
Cela signifie pour vous, que vous pouvez ignorer si un contrôle doit être exécuté sur le serveur de configuration ou le serveur de dépôt.

.Schéma d'un serveur autonome OPSI
image::../images/opsi-config-server.png["opsi config server",width=200]

OPSI avec serveurs de dépôts multiples: +
Si vous avez un gestion centralisée d'un environnement OPSI de plusieurs emplacements (un serveur de config, plusieurs serveurs de dépôt) la situation est plus compliquée. Donc, vous devez comprendre la situation:

.Schéma d'un environnement OPSI avec multiples dépôt 
image::../images/central-config-server.png["opsi multi depot environment",width=200]

Comme le souligne la figure il n'existe qu'un seul serveur qui contien les données de configuration - le backend. Il s'agit du serveur de configuration d'OPSI. Le serveur de dépôt OPSI, il ne dispose pas de son propre stockage de données, mais d'une rediréction vers le backend. Cela signifie que si vous demandez à un serveur de dépôt pour les données de configuration, cette question sera redirigé vers le serveur de configuration. Et cela conduit à la conséquence que chaque contrôle qui va à l'encontre du backend sera au moins exécuter sur le serveur de configuration. Vous devez donc répondre à des contrôles qui s'exécutent à l'encontre du backend toujours sur le serveur de configuration. Même dans la situation de collecte d'informations sur les clients qui sont affectés à un dépôt qui est différente du serveur de configuration et le contrôle est logiquement partie de la vérification de ce serveur de dépôt.

Si vous exécutez les contrôles directs vous avez l'habitude aussi d'aborder la configuration du serveur opsi. Vous pouvez régler le serveur de dépôt si les clients ne peuvent pas être atteint par le serveur de configuration OPSI via le port 4441. Dans ce cas, c'est une bonne idée de s'adresser au serveur de dépôt.

.Contrôles distribués
image::../images/verteilte_checks_en.png["opsi distributed checks",width=400]

[[opsi-Nagios-Connector-checks-plugin]]
==== opsi-check-plugin

Au niveau du serveur nagios il n'existe qu'un seul opsi-check-plugin qui fournit un large éventail de différents contrôles. En fonction du nombre de fonctionnalités il y a aussi un grand nombre d'options de ligne de commande. Lister toutes ces options ne vous aidera pas trop. Au lieu de cela, l'option sera expliqué dans le contexte de la documentation des possibles contrôles. +
Toutefois, pour obtenir une liste de toutes les options vous pouvez utiliser `check_opsi` avec les paramètres `--help` ou `-h`.

Les options générales suivantes sont nécessaires pour chaque contrôle:

.General Options
|=======================
|Option|Description|Exemple
|-H,--host|serveur opsi qui devrait exécuter le contrôle|configserver.domain.local
|-P,--port|opsi webservice port|4447 (Default)
|-u,--username|opsi monitoring user|monitoring
|-p,--password|opsi monitoring password|monitoring123
|-t,--task|opsi check method (case sensitive)|
|=======================

Le chapitre suivant décrit comment appeler opsi-check-plugin en ligne de commande. Comment vous devez configurer ces appels à votre serveur Nagios est décrit au chapitre 'configuration'.

Afin d'installer opsi-check-plugin sur votre serveur Nagios vous devez ajouter le dépôt OPSI sur votre serveur et installer le paquet 'opsi-nagios-plugins'. +
Par exemple, dans Debian ou Ubuntu avec les commandes suivantes:

[source,prompt]
----
apt-get install opsi-nagios-plugins
----

Pour le moment ce paquet n'est disponible que pour Debian/Ubuntu. Les paquets pour d'autres distributions suivront plus tard. Néanmoins, le plugin lui-même est écrit en python et devrait fonctionner sur n'importe quelle distribution.

Le paquet se base sur le paquet 'nagios-plugins-basic' et installe le plugin dans `/usr/lib/nagios/plugins`. +
Selon la flexibilité de check_plugin il n'y a pas de configuration automatique.

[[opsi-Nagios-Connector-checks-opsiWebservice]]
===== Contrôle: opsi web service 

Cette vérification surveille le processus opsi web service (opsiconfd). Cette vérification renvoie également les données de performance. Vous devez exécuter cette vérification sur chaque serveur OPSI parce que chaque serveur OPSI dispose d'un processus opsiconfd.

[source,prompt]
----
check_opsi.py -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiWebservice
----

Cette vérification renvoi normalement OK. +
Vous obtiendrez d'autres valeurs de retour dans les situations suivantes:

* Critical: +
** Si opsiconfd a des problèmes et ne peut pas répondre correctement.
** Si opsiconfd consomme plus de 80% de la cpu.
** Si vous avez un taux d'erreurs RPC de plus de 20%.

* Warning: +
** Si opsiconfd consomme plus de 60% (mais moins de 80%) de la cpu.
** Si vous avez un taux d'erreurs RPC de plus de 10% mais moins de 20%

* Unknown: +
Le service web OPSI n'a pas pu être atteint.

NOTICE:
La valeur en pourcentage de la consommation cpu appartient toujours à une cpu parce que opsiconfd peut seulement utiliser une cpu. (Cela pourrait changer avec l'extension de multi-traitement d'opsi)

[[opsi-Nagios-Connector-checks-opsiWebservice-pnp4nagios-template]]
===== Contrôle: opsi web service pnp4nagios template

Pour l'affichage des données de performance il y a un modèle pour pnp4nagios qui affiche les données d'une manière combinée. +
Ici, n'est pas décrit comment installer pnp4nagios. Nous supposons que pnp4nagios est installé et configuré correctement. La façon dont vous avez à utiliser pour configurer notre modèle peut différer de la façon ci-dessous décrite en fonction de votre installation pnp4nagios (qui peut utiliser un chemin différent).

Les modèles standard affichent pour toutes les données de performance un diagramme propre. Pour créer un affichage combiné vous devez passer aux étapes suivantes:

Étape 1: +
créez dans `/etc/pnp4nagios/check_commands` un fichier nommé  `check_opsiwebservice.cfg` et insérez le contenu suivant:

[source,cmd]
----
CUSTOM_TEMPLATE = 0
DATATYPE = ABSOLUTE,ABSOLUTE,ABSOLUTE,ABSOLUTE,DERIVE,GAUGE,GAUGE,GAUGE
----

Étape 2: +
accédez au répertoire `/usr/share/pnp4nagios/html/templates` et placez dedans le fichier `check_opsiwebservice.php` que vous pouvez consulter à partir de svn.opsi.org:

[source,cmd]
----
cd /usr/share/pnp4nagios/html/templates
svn co https://svn.opsi.org/opsi-pnp4nagios-template/trunk/check_opsiwebservice.php
----

S'il vous plaît vérifiez que votre fichier php est nommé exactement comme le 'command_name' qui est défini dans `/etc/nagios3/conf.d/opsi/opsicommands.cfg`. Si les noms ne correspondent pas, un modèle standard sera utilisé à la place de notre modèle combiné.

Après l'installation de ce modèle vous devez supprimer la bases de données RRD qui appartiennent à ce contrôle (s'il y a une existante). Vous trouverez ces bases de données dans `/var/pnp4nagios/perfdata/<host>/` où vous devriez (seulement) supprimer les fichiers `opsi-webservice.rrd` et `opsi-webservice.xml`.

Si vous avez tout configuré correctement vous devriez maintenant pouvoir voir les diagrammes comme dans la capture d'écran ci-dessous.


image::../images/pnp4nagios.png["uib template for pnp4nagios",width=400]

[[opsi-Nagios-Connector-checks-opsidiskusage]]
===== Contrôle: opsi-check-diskusage

Ce contrôle surveille l'utilisation des ressources (répertoires) qui sont utilisées par OPSI. Le tableau suivant montre les noms des ressources et les répertoires correspondants:

.ressources opsi
|=======================
|Resource name|Path
|/|/usr/share/opsiconfd/static
|configed|/usr/lib/configed
|depot|/opt/pcbin/install
|repository|/var/lib/opsi/repository
|=======================

S'il vous plaît notez que ce contrôle surveille uniquement les données pertinentes d'opsi et remplace une vérification général de l'usage du disque pour le serveur.

La commande suivante extrait toutes les ressources en même temps:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDiskUsage
----

En plus de cette variante standard vous pouvez restreindre la vérification à la ressource `depot`:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDiskUsage -r depot
----

La valeur par défaut du résultat de cette vérification est 'OK' et l'espace libre des ressources. L'espace libre est donnée en Gigabyte. Les valeurs par défaut pour les résultats de 'Warning' et 'Critical' sont:

* WARNING: Si au moins une ressource as 5GB ou moins d'espace libre.
* CRITICAL: Si au moins une ressource as 1GB ou moins d'espace libre.

Ce sont les seuils par défaut. Elles peuvent changer en donnant d'autres valeurs pour 'Warning' avec les options -W ou --warning et pour 'Critical' avec les options -C ou --critical. Avec ces options, vous pouvez indiquer les seuils en Gigabyte (G) et en pourcentage (%) ainsi. La sortie produite utilise la même unité qui est utilisé pour définir les seuils. +
Enfin un exemple:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDiskUsage -r depot --warning 10% --critical 5%
----

[[opsi-Nagios-Connector-checks-clientstatuscheck]]
===== Contrôle: opsi-client-status 

L'un des objectifs du connecteur OPSI Nagios est la surveillance du déploiement de logiciel par l'écoute des clients individuels. C'est l'un des contrôles, qui est conçu pour cet emploi. Plus exactement: les situations 'déploiement du logiciel' et 'vu la dernière fois' d'un certain client sont vérifiées.

Le résultat des contrôles suivants est déterminée par deux états différents:

* L'état de déploiement d'un ou plusieurs logiciels: +
Le résultat de l'état du déploiement de logiciels peut être: 
** 'OK' si le logiciel est installé dans le même produit et version du paquet qui est disponible au niveau du serveur et aucune demande d'action est définie.
** 'Warning' si le logiciel est installé dans une version qui est différente de la version serveurs ou si une demande d'action est définie.
** 'Critical' s'il y a un 'échec' rapporté par la dernière action.

* Le temps écoulé depuis 'vu la dernière fois': +
Le résultat du temps écoulé depuis 'vu la dernière fois' peut être:
** 'OK' si le client a été vu moin ou égale à 30 jours avant.
** 'Warning' si le client a été vu plus de 30 jours avant.


Ce contrôle peut être utilisé dans différentes variantes, ici la plus simple, qui comprend tous les logiciels:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkClientStatus -c opsiclient.domain.local
----

Comme variante, il est possible d'exclure des produits de la vérification. par exemple:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkClientStatus -c opsiclient.domain.local -x firefox
----

Dans l'exemple ci-dessus le produit 'firefox' a été exclu de la vérification. Donc, cette vérification ne passera pas à critique parce que la dernière action sur 'firefox' a signalé une défaillance.

[[opsi-Nagios-Connector-checks-opsiproductstatus]]
===== Contrôle: opsi-check-ProductStatus

Un autre objectif du connecteur OPSI Nagios est la surveillance du déploiement de logiciel par l'écoute de produit unique ou de groupe de produits.

Le résultat de ces contrôles est déterminé par les états suivants:

Le résultat de l'état du déploiement de logiciels peut être: 
* 'OK' si le logiciel est installé dans le même produit et version du paquet qui est disponible au niveau du serveur et aucune demande d'action est définie.
* 'Warning' si le logiciel est installé dans une version qui est différente de la version serveurs ou si une demande d'action est définie.
* 'Critical' s'il y a un 'échec' rapporté par la dernière action.


This checks has many variants and is so very flexible. The bast way to explain these variants are examples.

The simplest variant check one product on all clients. Here you have to give the product as the opsi `productId`.

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox
----

In a simple one server opsi environment, this check is all you need to check the state of the product 'firefox' on every client. +
You will get the information how many clients are in 'Warning' and in 'Critical' states. 

To get the information which clients exactly have the problems, you should call the check in the `verbose mode`:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox -v
----

An other variant is, that you may exclude a client from the check.

//// produkt muss angegebn werden ?! ////
[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox -x client.domain.local
----

In a opsi environment with multiple depot servers you have to use additional options to check also the clients that are not assigned to the config servers depot. If you have multiple depots, you may give the depots name as parameter:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox -d depotserver.domain.local
----

The reason is that the version of the software packages may differ between your depots. So every client has to be checked against the versions at the depot where they are assigned to. An advantage is that can place the display of the results to the depot server. +
You may give instead of the depot servers name the keyword 'all' which means all known depot servers. But this normally make only sense if you have only one or two depots. You may also give a comma separated list of depot servers.

An other way to define the checks is to give the name of a opsi groups. So you may check the software roll out state of all products in a given opsi product group. If you have for example a product group 'accounting' you may use the following call:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -g accounting
----

Now you will check all products that are Members of the opsi product group 'accounting' by this single check. Important is to see, that the resolution of the opsi group is done while the check at the opsi server. So you may change the opsi group at the opsi Management interface and so you will change the products that will checked without any changes at the Nagios server.

NOTE: Sub groups (groups in groups) will not be resolved.

In the same way it is possible to define the clients that should be checked by giving the name of a opsi client group. +
An example for a client group 'productiveclients':

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -g accounting -G productiveclients
----

This would check all products of the product group 'accounting' at all clients of the client group 'productiveclients'. 

NOTE: Sub groups (groups in groups) will not be resolved.

NOTE: You may also give a comma separated list of opsi groups.

[[opsi-Nagios-Connector-checks-opsidepotsync]]
===== Check: opsi-check-depotsync 

If you are using multiple opsi depots the monitoring of synchronicity is important. Even if your depots are for good reasons not completely synchronize they should be synchrony as much as possible to avoid problems by moving a client from one depot to another.

This check monitors if your depots are synchronize according to product ids, product versions and package versions.

This check returns:

* 'OK' +
If all is in sync.
* 'Warning' +
If there is any difference

You should run this check always on the config server because all the data come from the backend of the config server.

Here are some examples.

The base variant:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus
----

This base variant is equivalent to the following call:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d all
----

So if you don't give the depots which are have to be checked, all known depots will be checked. If you have a lot of depots the interpretation of the result is complicated, so it is a good idea to define a lot of single checks where the depots are given as comma separated list:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local
----

With this check you compare all products, that are installed *on both* depots. Any product which is installed only on one of the depot is ignored and will not effect the result.

If you want to include products which are not installed on all checked depots, you have to use the `strictmode` switch:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local --strictmode
----

Now also differences about missing products will be seen.

If you like to exclude a product from the check (perhaps because this product should be in different versions on different depots) you may do this by using the `-x` option. Here you may also use a comma separated list:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local --strictmode -x firefox,thunderbird
----

This check will not warn if the products 'firefox' or 'thunderbird' or not in sync.

Instead of excluding products you may give an explicit list of products that has to been checked:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local --strictmode -e firefox,thunderbird
----

In this case *only* 'firefox' and 'thunderbird' will be checked. We recommend to use this check variant with `strictmode` to see if one of the products is missing.


[source,prompt]
----
./check_opsi.py -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSync
----

[[opsi-Nagios-Connector-checks-pluginOnClient]]
===== Check: nagios client plugin check via opsiclientd

This check gives you an easy possibility to integrate checks that collects the data direct on the client with a minimum of configuration work.

So this check tells the opsiclientd which is running at the opsi client to run a command, fetch the output and send it back.

This extension is not intended to be a complete replacement of a full featured Windows Nagios agent. It is only a light weight alternative.

The plugins which the opsiclientd may call must be compatible to the `Nagios plug-in development guidelines`. (More details at: http://nagiosplug.sourceforge.net/developer-guidelines.html ).

In order to run such a plugin on the client, it has to be installed at the client. This problem you will solve by deploying it as an opsi package. The path where the plugin is installed at client doesn’t matter because you have to give the complete path at check definition. We recommend to install all plugins in one directory to ease the maintenance of the plugins at the client.

For security reasons you should make sure that non privileged users have no write access to the plugins, because they will be executed from the opsiclientd with 'system' privileges.

There are lot of ready to use plugins at the internet. One important address to look is: +
http://exchange.nagios.org/

In the following we assume that your plugins are installed at `C:\opsi.org\nagiosplugins\` and we will find ther the plugin `check_win_disk.exe` out of the package 'nagioscol' from +
http://sourceforge.net/projects/nagiosplugincol/

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkPluginOnClient --plugin "C:\\opsi.org\\nagiosplugincol\\check_win_disk.exe C:" -c client.domain.local
----

This call checks the client `client.domain.local`. At the client the plugin `check_win_disk.exe` is called with the parameter `C:`. This means, that the hard drive with the letter 'C' should be checked. The output and the result value of the plugin will be fetched by the opsiclientd and will be given back to the Nagios server (via the opsi server) in a for Nagios correct format.

Another special feature is to hold the last check results, even if the client is not reachable.

This feature was implemented according to the fact that desktop clients not always are running like servers, but the most time in their life are usually switched off. Normally Nagios will show for switched off clients the result 'Unknown'. In fact the most problems on the monitored clients will not disappear by just switching them off and on again. So the information that a client had a problem before it was switched off may be an essential information for the system administrator. (You may try to solve this problem by using `Timeperiods` at the Nagios configuration, but we think that this is not flexible enough and leads to a permanent configuration work). So this opsi extension give you the possibility to give back the last real check results if the client is not reachable right now.

In order to use this feature, you have to use the Nagios macros `$SERVICESTATEID$` and `$SERVICEOUTPUT$`. `$SERVICESTATEID$` gives the last result value and should be passed to the `-s` Option. `$SERVICEOUTPUT$` gives the last output line and should be passed to the `-o` Option. So check can give these last values instead of 'Unknown' if the client is not reachable.


[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkPluginOnClient --plugin "C:\\opsi.org\\nagiosplugincol\\check_win_disk.exe C:" -c client.domain.local -s $SERVICESTATEID$ -o  $SERVICEOUTPUT$
----

[[opsi-Nagios-Connector-configuration]]
=== opsi monitoring configuration

This chapter focuses on the configuration that have to been made for a working interface between the opsi and the Nagios server. Just see this as a recommendation, there will be  a lot of other ways to do the job.

This description uses a Nagios server as monitoring server. On a Icinga server it should work very similar but you have to change some path entries. It should also work on other Nagios derivatives but this is not tested.

[TIP]
====
The configurationfiles from these Chapter are in opsi-nagios-connector-utils svn-Repository. To get these example configurationfiles you can connect over a browser to following url:

[source,prompt]
----
https://svn.opsi.org/listing.php?repname=opsi-nagios-connector-utils
----
or you can make a direct checkout from repository with following command:
[source,prompt]
----
svn co https://svn.opsi.org/opsi-nagios-connector-utils
----
====

[[opsi-Nagios-Connector-configuration-User]]
==== opsi monitoring user

In monitoring environments you will often find that the access is just restricted by IP numbers. Because of the lack of security of this solution we decided to work with a real user / password security in this opsi extension.

Using the opsi standard group `opsiadmin` would give the Nagios more rights than needed. So you have to create an own opsi user for the Connecteur opsi-Nagios.

In the following example a user named 'monitoring' with the password 'monitoring123' is created for opsi:

[source, prompt]
----
opsi-admin -d method user_setCredentials monitoring monitoring123
----

The created user 'monitoring' will be stored with its encrypted password at the `/etc/opsi/passwd` and is not a user which may be used to login at a shell. In fact it is no real Unix user.

You have to create this user only on your config server, even if you have multiple depots.

At your Nagios server you should mask the user and password by making an entry at the `/etc/nagios3/resource.cfg`. This should look for example like this:

[source, prompt]
----
$USER2$=monitoring
$USER3$=monitoring123
----

The number behind '$USER' may vary. If this configuration was not used before, there should be only `$USER1$` be used. According to what you are using here, you might have to change the other examples in this manual.

[[opsi-Nagios-Connector-configuration-directory]]
==== Connecteur opsi-Nagios configuration directory

To make the maintenance of the Nagios configuration easier, we recommend to put all 'opsi nagios connector' related configuration files in one separated place. +
So just create below `/etc/nagios3/conf.d` a new directory named `opsi` for these configurations.

The configuration files we will place in this directory are:

* Nagios Template: `opsitemplates.cfg`
* Hostgroups: `opsihostgroups.cfg`
* Server Hosts: `<full name of the server>.cfg`
* Commands: `opsicommands.cfg`
* Contacts: `opsicontacts.cfg`
* Services: `opsiservices.cfg`

All the client configuration files we recommend to put in sub directory of this place. Therefore you create below `/etc/nagios3/conf.d/opsi` another directory named `clients`.

[[opsi-Nagios-Connector-configuration-template]]
==== Nagios template:  `opsitemplates.cfg`

Using templates is a standard functionality of Nagios which will not explained here. The main advantage is that it makes the single configuration files smaller and easier to read (and write). 

Inside of the templates we use some Nagios 'custom variables' for often used values. According to the fact, that the most checks have to run on the opsi config server, we will define the name and port of the config server as such a 'custom variable':

[source,opsifiles]
----
_configserver           configserver.domain.local
_configserverurl        4447
----
You will find this below in the template definitions. +
These 'custom variables' may later on be referenced by the Nagios macros: `$_HOSTCONFIGSERVER$` and `$_HOSTCONFIGSERVERPORT$`. (These macros have leading 'HOST' in their name, because they are defined inside of a host definition).

For more details on variable and macro take look at your Nagios documentation.

Now the first file we create in `/etc/nagios3/conf.d/opsi` is the template definition file `opsitemplates.cfg`.

This file may hold different templates. Every template is created according to the following patter (which contains comments for better understanding):
[source, opsifiles]
----
define host{
        name                    opsihost-tmp    ; The name of this host template
        notifications_enabled           1       ; Host notifications are enabled
        event_handler_enabled           1       ; Host event handler is enabled
        flap_detection_enabled          1       ; Flap detection is enabled
        failure_prediction_enabled      1       ; Failure prediction is enabled
        process_perf_data               0       ; Process performance data
        retain_status_information       1       ; Retain status information across program restarts
        retain_nonstatus_information    1       ; Retain non-status information across program restarts
                max_check_attempts              10
                notification_interval           0
                notification_period             24x7
                notification_options            d,u,r
                contact_groups                  admins
        register                        0       ; DONT REGISTER THIS DEFINITION - ITS NOT A REAL HOST, JUST A TEMPLATE!
        icon_image                      opsi/opsi-client.png
        }
----
NOTE:
* The optional option icon_image may put it to an image with relative path below: `/usr/share/nagios3/htdocs/images/logos/`.
* Optional you may give an own 'contact_group', which have to be defined as contact object, for example in the file `opsicontacts.cfg`.


Now we recommend to create templates for the following objects

* opsi server
* opsi client
* opsi service
* and 2 templates for pnp4nagios (host-pnp / srv-pnp)

Let's start with the example of the opsi server template:
[source,opsifiles]
----
define host{
        name                            opsi-server-tmpl
        notifications_enabled           1
        event_handler_enabled           1
        flap_detection_enabled          1
        failure_prediction_enabled      1
        process_perf_data               1
        retain_status_information       1
        retain_nonstatus_information    1
                check_command                   check-host-alive
                max_check_attempts              10
                notification_interval           0
                notification_period             24x7
                notification_options            d,u,r
                contact_groups                  admins,opsiadmins
        _configserver                   configserver.domain.local
        _configserverport               4447
        register                        0
        icon_image                      opsi/opsi-client.png
        }
----

You just have to change 'configserver.domain.local' to your config server name. Also you may change the 'contact_groups' to your needs.

The next part of the file `opsitemplates.cfg` is the template for the clients:
[source,opsifiles]
----
define host{
        name                            opsi-client-tmpl
        notifications_enabled           1
        event_handler_enabled           1
        flap_detection_enabled          1
        failure_prediction_enabled      1
        process_perf_data               1
        retain_status_information       1
        retain_nonstatus_information    1
                max_check_attempts              10
                notification_interval           0
                notification_period             24x7
                notification_options            d,u,r
                contact_groups                  admins,opsiadmins
        _configserver                   configserver.domain.local
        _configserverport               4447
        register                        0
        icon_image                      opsi/opsi-client.png
        }
----
The Option "check command	check-host-alive" should be not set here because the clients are not always running. In effect the clients will be displayed as 'pending' instead of 'offline'. 

You just have to change 'configserver.domain.local' to your config server name. Also you may change the 'contact_groups' to your needs.

The next part of the file `opsitemplates.cfg` is the template for the opsi-services:
[source,opsifiles]
----
define service{
        name                            opsi-service-tmpl
        active_checks_enabled           1
        passive_checks_enabled          1
        parallelize_check               1
        obsess_over_service             1
        check_freshness                 0
        notifications_enabled           1
        event_handler_enabled           1
        flap_detection_enabled          1
        failure_prediction_enabled      1
        process_perf_data               1
        retain_status_information       1
        retain_nonstatus_information    1
                notification_interval           0
                is_volatile                     0
                check_period                    24x7
                normal_check_interval           5
                retry_check_interval            1
                max_check_attempts              4
                notification_period             24x7
                notification_options            w,u,c,r
                contact_groups                  admins,opsiadmins
        register                        0
        }
----

If you are using pnp4nagios for the graphic display of performance data you will need two other templates in the file `opsitemplates.cfg`:

[source,opsifiles]
----
define host {
   name       host-pnp
   action_url /pnp4nagios/index.php/graph?host=$HOSTNAME$&srv=_HOST_
   register   0
}

define service {
   name       srv-pnp
   action_url /pnp4nagios/index.php/graph?host=$HOSTNAME$&srv=$SERVICEDESC$
   register   0
}
----

[[opsi-Nagios-Connector-configuration-hostobjects-groups]]
==== opsi hostgroup:  `opsihostgroups.cfg`

The nest step is to define the hostgroups. This helps to structure the display of the results as well as the service definitions.

So create a file named `opsihostgroups.cfg` wit the following content:

[source,opsifiles]
----
define hostgroup {
        hostgroup_name  opsi-clients
        alias           OPSI-Clients
}

define hostgroup {
        hostgroup_name  opsi-server
        alias           OPSI-Server
        members         configserver.domain.local, depotserver.domain.local
}
----
Do not forget to edit the 'member' line.

[[opsi-Nagios-Connector-configuration-hostobjects-server]]
==== opsi server:  `<full name of the server>.cfg`

The next step is to create for every opsi server you are running an own configuration file. This file should be named based on the pattern `<full name of the server>.cfg`. For example `configserver.domain.local.cfg`. +
(You may also create one file (e.g. `opsihost.cfg` with all server definitions). +
The content should look like this:

[source,opsifiles]
----
define host{
        use				opsi-server-tmpl
        host_name		configserver.domain.local
        hostgroups		opsi-server
        alias				opsi Configserver
        address			configserver.domain.local
        }

define host{
        use				opsi-server-tmpl
        host_name		depotserver.domain.local
        hostgroups		opsi-server
        alias				opsi Depotserver
        address			depotserver.domain.local
        }
----

Explanation of the entries:
* 'use' references to the used template.
* 'hostgroups' tells us to which hostgroup this server belongs.

[[opsi-Nagios-Connector-configuration-hostobjects-clients]]
==== opsi Clients:  `clients/<full name of the client>.cfg`

The opsi client configurations should be placed in an own sub directory. They should be defined like this:

[source,opsifiles]
----
define host{
        use				opsi-client-tmpl
        host_name		client.domain.local
        hostgroups		opsi-clients
        alias				opsi client
        address			client.domain.local
        _depotid			depotserver.domain.local
        }
----

This client configuration uses again a 'custom variable': `_depotid`. This 'custom variable' may be referenced by the macro `$_HOSTDEPOTID$`. +
The usage is optional. If a client may be not connected by the opsi configuration server directly, you will here write down from which depot server the client can be contacted.

To make it easier to create the configuration files for your large number of opsi clients, you may run the following script on your opsi configuration server:

[source,prompt]
----
#!/usr/bin/env python

from OPSI.Backend.BackendManager import *

template = '''
define host {
        use             opsi-client-tmpl
        host_name       %hostId%
        hostgroups      opsi-clients
        alias           %hostId%
        address         %hostId%
        }
'''

backend = BackendManager(
             dispatchConfigFile = u'/etc/opsi/backendManager/dispatch.conf',
             backendConfigDir   = u'/etc/opsi/backends',
             extensionConfigDir = u'/etc/opsi/backendManager/extend.d',
                        )


hosts = backend.host_getObjects(type="OpsiClient")

for host in hosts:
        filename = "%s.cfg" % host.id
        entry = template.replace("%hostId%",host.id)
        f = open(filename, 'w')
        f.write(entry)
        f.close()
----


[[opsi-Nagios-Connector-configuration-commands]]
==== opsi command configuration:  `opsicommands.cfg`

Now we have to define which of the check commands, which are described before, we want to use. You should do this in a file named `opsicommands.cfg`. +
This is just an example which you may change to your needs:

First let us explain the structure of the entries:

[source,opsifiles]
----
define command{
        command_name	check_opsi_clientstatus
        command_line		$USER1$/check_opsi -H $_HOSTCONFIGSERVER$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkClientStatus -c $HOSTADDRESS$
        }
----

The `command_name` will be used by other configuration files. The option `command_line` defines the command and all used arguments.

Based on this pattern we create now the file `opsicommands.cfg`:

[source,opsifiles]
----
define command {
        command_name    check_opsiwebservice
        command_line    /usr/lib/nagios/plugins/check_opsi -H $HOSTADDRESS$ -P 4447 -u $USER2$ -p $USER3$ -t checkOpsiWebservice
}
define command {
        command_name    check_opsidiskusage
        command_line    /usr/lib/nagios/plugins/check_opsi -H $HOSTADDRESS$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkOpsiDiskUsage
}
define command {
        command_name    check_opsiclientstatus
        command_line    /usr/lib/nagios/plugins/check_opsi -H $_HOSTCONFIGSERVER$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkClientStatus -c $HOSTADDRESS$
}
define command {
        command_name    check_opsiproductstatus
        command_line    /usr/lib/nagios/plugins/check_opsi -H $_HOSTCONFIGSERVER$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkProductStatus -e $ARG1$ -d $HOSTADDRESS$ -v
}
define command {
        command_name    check_opsiproductStatus_withGroups
        command_line    /usr/lib/nagios/plugins/check_opsi -H $_HOSTCONFIGSERVER$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkProductStatus -g $ARG1$ -G $ARG2$ -d "all"
}
define command {
        command_name    check_opsiproductStatus_withGroups_long
        command_line    /usr/lib/nagios/plugins/check_opsi -H $_HOSTCONFIGSERVER$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkProductStatus -g $ARG1$ -G $ARG2$ -v -d "all"
}
define command {
        command_name    check_opsidepotsync
        command_line    /usr/lib/nagios/plugins/check_opsi -H $_HOSTCONFIGSERVER$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkDepotSyncStatus -d $ARG1$
}
define command {
        command_name    check_opsidepotsync_long
        command_line    /usr/lib/nagios/plugins/check_opsi -H $_HOSTCONFIGSERVER$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkDepotSyncStatus -d $ARG1$ -v
}
define command {
        command_name    check_opsidepotsync_strict
        command_line    /usr/lib/nagios/plugins/check_opsi -H $_HOSTCONFIGSERVER$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkDepotSyncStatus -d $ARG1$ --strict
}
define command {
        command_name    check_opsidepotsync_strict_long
        command_line    /usr/lib/nagios/plugins/check_opsi -H $_HOSTCONFIGSERVER$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkDepotSyncStatus -d $ARG1$ --strict -v
}
define command {
        command_name    check_opsipluginon_client
        command_line    /usr/lib/nagios/plugins/check_opsi -H $_HOSTCONFIGSERVER$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkPluginOnClient -c $HOSTADDRESS$ --plugin $ARG1$
}
define command {
        command_name    check_opsipluginon_client_with_states
        command_line    /usr/lib/nagios/plugins/check_opsi -H $_HOSTCONFIGSERVER$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkPluginOnClient -c $HOSTADDRESS$ --plugin $ARG1$ -s $SERVICESTATEID$ -o "$SERVICEOUTPUT$"
}
define command {
        command_name    check_opsipluginon_client_from_depot
        command_line    /usr/lib/nagios/plugins/check_opsi -H $_HOSTDEPOTID$ -P $_HOSTCONFIGSERVERPORT$ -u $USER2$ -p $USER3$ -t checkPluginOnClient -c $HOSTADDRESS$ --plugin $ARG1$
}
----

[[opsi-Nagios-Connector-configuration-contacts]]
==== Contacts:  `opsicontacts.cfg`

This define the contacts which will get notifications.

[source,opsifiles]
----
define contact{
        contact_name                    adminuser
        alias                           Opsi
        service_notification_period     24x7
        host_notification_period        24x7
        service_notification_options    w,u,c,r
        host_notification_options       d,r
        service_notification_commands   notify-service-by-email
        host_notification_commands      notify-host-by-email
        email                           root@localhost
        }
define contactgroup{
        contactgroup_name       opsiadmins
        alias                   Opsi Administrators
        members                 adminuser
        }
----
You should replace 'adminuser' by one or more real users.

[[opsi-Nagios-Connector-configuration-services]]
==== Services:  `opsiservices.cfg`

Finally we define with the 'services' what the Nagios server have to monitor and to display. This definition are using the definition of the other configuration file above like templates, commands and hostgroups or hosts.

As first part we define the services which give us information's about the servers. One of these is the check if the depots are in sync, which is here down against 'all' known depots.

[source,opsifiles]
----
#OPSI-Services
define service{
        use                             opsi-service-tmpl,srv-pnp
        hostgroup_name                  opsi-server
        service_description             opsi-webservice
        check_command                   check_opsiwebservice
        check_interval                  1
        }
define service{
        use                             opsi-service-tmpl
        hostgroup_name                  opsi-server
        service_description             opsi-diskusage
        check_command                   check_opsidiskusage
        check_interval                  1
        }
define service{
        use                             opsi-service-tmpl
        hostgroup_name                  opsi-server
        service_description             opsi-depotsyncstatus-longoutput
        check_command                   check_opsidepotsync_long!all
        check_interval                  10
        }
define service{
        use                             opsi-service-tmpl
        hostgroup_name                  opsi-server
        service_description             opsi-depotsyncstatus-strict-longoutput
        check_command                   check_opsidepotsync_strict_long!all
        check_interval                  10
        }
----

The next part is the monitoring of the software roll out. In one check a concrete opsi product 'opsi-client-agent' is mentioned. In two other check are referenced on a opsi product group 'opsiessentials' and opsi client group 'productiveclients'.

[source,opsifiles]
----
define service{
        use                             opsi-service-tmpl
        hostgroup_name                  opsi-clients
        service_description             opsi-clientstatus
        check_command                   check_opsiclientstatus
        check_interval                  10
        }
define service{
        use                             opsi-service-tmpl
        hostgroup_name                  opsi-server
        service_description             opsi-productstatus-opsiclientagent
        check_command                   check_opsiproductstatus!opsi-client-agent
        check_interval                  10
        }
define service{
        use                             opsi-service-tmpl
        hostgroup_name                  opsi-server
        service_description             opsi-productstatus-opsiessentials-group
        check_command                   check_opsiproductStatus_withGroups!opsiessentials!productiveclients
        check_interval                  10
        }
define service{
        use                             opsi-service-tmpl
        hostgroup_name                  opsi-server
        service_description             opsi-productstatus-opsiessentials-group-longoutput
        check_command                   check_opsiproductStatus_withGroups_long!opsiessentials!productiveclients
        check_interval                  10
        }
----

In the third and last part of the file, the checks which are should run directly on the clients ('direct checks') are defined. +
These checks are (for example) not assigned to hostgroups but to single hosts or lists of hosts ('client.domain.local','depotclient.domain.local').

Some description:

* 'opsi-direct-checkpluginonclient' +
runs a normal 'direct' check on the client and results to 'unknown' if the client is offline. +
At this check the config server try’s to reach the client directly.

* 'opsi-direct-checkpluginonclient-with-servicestate' +
is equal to 'opsi-direct-checkpluginonclient', but returns the last valid result if the client is offline (instead of 'unknown')
* 'opsi-direct-checkpluginonclient-from-depot' +
is equal to 'opsi-direct-checkpluginonclient', but the client will be connected by the server which is given in the host configuration as '_depotid'.

[source,opsifiles]
----
define service{
        use                             opsi-service-tmpl
        host_name                       client.domain.local,depotclient.domain.local
        service_description             opsi-direct-checkpluginonclient
        check_command                   check_opsipluginon_client!"C:\\opsi.org\\nagiosplugins\\check_memory.exe"
        check_interval                  10
        }
define service{
        use                             opsi-service-tmpl
        host_name                       client.domain.local
        service_description             opsi-direct-checkpluginonclient-with-servicestate
        check_command                   check_opsipluginon_client_with_states!"C:\\opsi.org\\nagiosplugins\\check_memory.exe"
        check_interval                  10
        }
define service{
        use                             opsi-service-tmpl
        host_name                       depotclient.domain.local
        service_description             opsi-direct-checkpluginonclient-from-depot
        check_command                   check_opsipluginon_client_from_depot!"C:\\opsi.org\\nagiosplugins\\check_memory.exe"
        check_interval                  10
        }
----


