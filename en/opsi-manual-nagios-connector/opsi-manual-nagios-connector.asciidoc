////
; Copyright (c) uib gmbh (www.uib.de)
; This documentation is owned by uib
; Until we found a better license:
; All rights reserved.
; credits: http://www.opsi.org/credits/
////

:Author:    uib gmbh
:Email:     info@uib.de
:Date:      13.01.2012
:Revision:  4.0
:toclevels: 6


include::../common/opsi_terms.asciidoc[]

= opsi-Nagios-Connector

[[opsi-Nagios-Connector-introduction]]
== Introduction

Beside client managment is monitoring one the central functions in a modern IT service management. With opsi you got a client management tool. For monitoring tasks there are other well known open source solutions. So we build for the monitoring tasks in opsi not a own monitoring tool but a interface to existing solutions. With this opsi extension we provide a connector to Nagios. +
In the following chapters the design and configuration of the opsi-nagios-connector is explained.

The opsi-nagios-Connector isn't stricly bound to Nagios. It is developed vor the use with Nagios and Icinga. It should also work with other Nagios derivates but this is wether tested nor supported.

The scope of this manual is the design and configuration of the opsi-Nagios-Connector. It is not a Nagios manual. You will need a running Nagios installation and the knowlege how to run Nagios.

[[opsi-Nagios-Connector-prerequires]]
== Preconditions

[[opsi-Nagios-Connector-prerequires-opsi]]
=== Preconditions at the opsi server and client

This extension is at the moment a co-funding project which means that until the complete development costs are payed by co-funders, they are only allowed to use by the co-funders or for evaluation purposes. If we have earned the development cost we will give these modules for everybody for free. +
see also +
http://uib.de/en/opsi_cofunding/index.html
http://www.opsi.org/en/statistics

So as precondition to use this extension you need as first an activation file.
For evaluation purpose you will get a temporary activation file if you ask for it in a mail at info@uib.de.

Technical preconditions are opsi 4.0.1 with the following package and product versions:

.Needed product and package versions
[options="header"]
|==========================
|opsi-Paket|Version
|opsi-client-agent|>=4.0.1-26
|opsiconfd|>=4.0.1.11-1
|python-opsi|>=4.0.1.38-1
|==========================

[[opsi-Nagios-Connector-prerequires-nagios]]
=== Preconditions at the Nagios server


As precondition you need a Nagios installation in the version 3.x or a Icinga Installation in the version 1.6 or higher. +
For graphical output of performance data a pnp4nagios installation is required.

Further information you will find at:

www.nagios.org

www.icinga.org

www.pnp4nagios.org

[[opsi-Nagios-Connector-concept]]
== Concept

The opsi-Nagios-Connector contains of two core components. At first we will discuss these core components.

[[opsi-Nagios-Connector-concept-webservice]]
=== opsi web service extension

The heart of the opsi-Nagios-Connector are extened features of the opsi web service. These web service extension make it possible to run checks via web service on the opsi server. So the nagios server calls checks via web service, which are executed on the opsi-server and the results come back to the nagios server via opsi web service. Tha advantage of this solution is that there is nearly nothing to do on the monitored opsi server.

The focus of the opsi web service extension lies on opsi specific checks like e.g. rollout monitoring. For the 'normal' server monitoring you should use still standard check methods. 

[[opsi-Nagios-Connector-concept-opsiclientd]]
=== opsi-client-agent extension

A other part of the opsi-Nagios-Connector is a extension of the opsi-client-agent. +
In a opsi environment on every managed client runs a opsi-client-agent. With this extension you may use the opsi-client-agent as nagios agent as well. But in fact not all features of a standard Nagios agent like `NSClient++` are implemented at the opsi-client-agent. You may use the opsi-client-agent to run command line programs and send back the output.
efern der Ergebnisse.
////
Wenn man nicht alle Funktionen, wie NSCA benötigt, sondern nur ein paar Standard-Checks per Plugin auf den Clients ausführen oder eine Reihe von eigenen Plugins auf den Clients benutzen möchte, kann man den opsi-client-agent dazu verwenden. 
////

If you need more features for the client monitoring you should rollout a standard agent like `NSClient++` via opsi.

The advantage of using the opsi-client-agent as nagios agent is, that you don't need a additional agent on the client and that you don't need any access data for the clients at the monitoring server. These data is not needed because all check run via the opsi server. This makes the configuration a lot more easier.

[[opsi-Nagios-Connector-checks]]
== opsi-checks

The following chapter explains the goals and configurations of the opsi-checks.

[[opsi-Nagios-Connector-checks-background]]
=== Some background information about where to run the checks

Monitoring administrators know the difference between active and passive checks. 

With the opsi-Nagios-Connector we get a new difference: direct and indirect.

* direct : +
The check which collects information about a client runs on that client, get the information direct from the client and sends the information back.

* indirect : +
The check which collects information about a client runs on the opsi server and get the information from the opsi configuration data which is stored in the opsi backend. So - this information may be different from the actual situation of the client.

A good example for a indirect check is the `check-opsi-client-status`. This check gives you for a given client information about pendig action request and reported failures of the opsi software depolyment. So this are information about the client from the opsi servers point of view. Therefore this check runs on the opsi server and is a indirect check. A check which runs on the client is a direct check.

For a correct distribution and configuration of the checks you have to analyze your opsi installation. +
Acoording to the flexibility of opsi many various opsi configurations are possible. So here we can only explain some typical situations. Of course we will get help for special sitations by our comercial support.

only one opsi server: +
The opsi stand alone installation is the sitation that you will find at the most opsi environments. At this installation the opsi config server functionality is at the same server like the (one and only) opsi depot server functionality. +
This means to you, that you may ignore if a check has to be run on the config server or the depot server.

.Scheme of a standalone opsi server
image::../images/opsi-config-server.png["opsi config server",width=200]

opsi with multiple depotservers: +
If you have a central management of a multi location opsi environment (one config server, multiple depot servers) the sitation is more complicated. So you have to understand the sitation:

.Scheme opsi multi depot environment
image::../images/central-config-server.png["opsi multi depot environment",width=200]

As the figure points out there is only one server which have data storage for the configuration data - the data backend. This is the opsi config server. The opsi depot server has no own data storage but a redireted backend. This means that if you ask a depot server for any configuration data, this question will be redireted to the config server. And this leads to the consequence that every check which runs against the opsi data backend will at least run on the config server. So you should address checks that run against the backend always to the config server. Even in the sitation if you are collecting information about clients which are assingned to a depot which is different from the config server and the check is logically part of the check of this depot server.

If you running direct checks you normally also address the config server. You may address the depot server if the clients can't be reached by the opsi config server via port 4441. In this case it is a good idea to address the depot server.

VERTEILTE Checks Bild

[[opsi-Nagios-Connector-checks-plugin]]
=== opsi-check-plugin

At the Nagiso server there is only one opsi-check-plugin which provides a wide range of different checks. According to the number of features there is also a big number of command lin options. So - just list all these options won't help you much. Instead the option will be explained in the context of documentation of the possible checks. +
How ever to get a listing of all options you may call `check_opsi` with the parameters `--help` or `-h`.

The following general options are needed for every check:

.General Options
|=======================
|Option|Description|Example
|-H,--host|opsi server which should run the check|configserver.domain.local
|-P,--port|opsi webservice port|4447 (Default)
|-u,--username|opsi monitoring user|monitoring
|-p,--password|opsi monitoring password|monitoring123
|-t,--task|opsi check method (case sensitive)|
|=======================

The following chapter describes how to call the opsi-check-plugin is called on the command line. How you have to configure these calls at your Nagios server is described at the chapter 'configuration'.

In order to install the opsi-check-plugin on your Nagios server you should add the opsi repository to your server and install the package 'opsi-nagios-plugins'. +
For example at Debian or Ubuntu with the following commands:

[source,prompt]
----
apt-get install opsi-nagios-plugins
----

At the moment this package is only aviable for Debian/Ubuntu. Packages for other distribution will follow later. Nevertheless the plugin it self is written in python and should ran at any distribution.

The package bases on the package 'nagios-plugins-basic' and installs the plugin to `/usr/lib/nagios/plugins`. +
According to the flexibility of the check_plugin there is no automatic configuration.

[[opsi-Nagios-Connector-checks-opsiWebservice]]
==== Check: opsi web service 

This check monitors the opsi web service process (opsiconfd). This check returns also performance data. You should run this check on every opsi server because every opsi server have a opsiconfd process.

[source,prompt]
----
check_opsi.py -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiWebservice
----

This check return normally OK. +
You will get other return values in the following situations:

* Critical: +
** If the opsiconfd is in trouble and can't reply correctly.
** If the opsiconfd consumes more than 80% of the cpu.
** If you have a rate of RPC errors of more than 20%.

* Warning: +
** If the opsiconfd consumes more than 60% (but less than 80%) of the cpu.
** If you have a rate of RPC errors of more than 10% but less than 20%

* Unknown: +
The opsi web service could not be reached.

NOTICE:
The percentage value of the cpu consumption belongs always to one cpu because the opsiconfd only may use one cpu. (This may change with the opsi multi processing extension)

[[opsi-Nagios-Connector-checks-opsiWebservice-pnp4nagios-template]]
===== Check: opsi web service pnp4nagios template

For the display of performance data there is a template for pnp4nagios which displays the data in a combined way. +
Here is not described how to install pnp4nagios. We assume that pnp4nagios is installed and configured correctly. The way you have to use to configure our template may differ from the below described way according to your pnp4nagios installation (which may use different path).

Standard templates dispay for every performance data a own diagram. To create a combined display you have to go the following steps:

Step 1: +
create at `/etc/pnp4nagios/check_commands` a file named  `check_opsiwebservice.cfg` and insert the following content:

[source,cmd]
----
CUSTOM_TEMPLATE = 0
DATATYPE = ABSOLUTE,ABSOLUTE,ABSOLUTE,ABSOLUTE,DERIVE,GAUGE,GAUGE,GAUGE
----

Setp 2: +
change to the directory `/usr/share/pnp4nagios/html/templates` and place there a file `check_opsiwebservice.php` which you check out from svn.opsi.org:

[source,cmd]
----
cd /usr/share/pnp4nagios/html/templates
svn co https://svn.opsi.org/opsi-pnp4nagios-template/trunk/check_opsiwebservice.php
----

Please check that your php file is named exactly like the 'command_name' which is defined at the `/etc/nagios3/conf.d/opsi/opsicommands.cfg`. If the names don't match, a standard template will be used instead our combined template.

After installing this template you should delete the RRD data bases which belong to this check (if there any existing). You will find these data bases at `/var/pnp4nagios/perfdata/<host>/` where you should (only) delete the `opsi-webservice.rrd` and `opsi-webservice.xml` files.

If you have configured everything correctly you should now able to see diagrams like the following screenshot.


image::../images/pnp4nagios.png["uib template for pnp4nagios",width=400]

[[opsi-Nagios-Connector-checks-opsidiskusage]]
==== Check: opsi-check-diskusage

This check monitors the usage of the resources (directories) which are used by opsi. The following table shows the resource names and the corrosponding directories:

.opsi Ressourcen
|=======================
|Ressource name|Path
|/|/usr/share/opsiconfd/static
|configed|/usr/lib/configed
|depot|/opt/pcbin/install
|repository|/var/lib/opsi/repository
|=======================

Please note that this check monitors only opsi relevant data and do replace a general disk usage check for the server.

The following command retrieves all resources at one time:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDiskUsage
----

In addition to this standard variant you may restrict the check to the resource `depot`:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDiskUsage -r depot
----

The default result valu of this check is 'OK' and the free space of the resources. The free space is given in Gigabyte. The default values for the 'Warning' and 'Critical' results are:

* WARNING: If at least one resource have 5GB or less free space.
* CRITICAL: If at least one resource have 1GB or less free space.

This are the default tresholds. They may changed by giving other values for 'Warning' with the -W or --warning optionen and for 'Critical' wit the -C or --critical option. With these options you can give the tresholds as Gigabyte (G) and as percent (%) as well. The prodused output uses the same unit which is used to define the tresholds. +
Finally a example:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDiskUsage -r depot --warning 10% --critical 5%
----

[[opsi-Nagios-Connector-checks-clientstatuscheck]]
==== Check: opsi-client-status 

One of the targets of the opsi Nagios connector is the software rollout monitoring by viewing to single clients. This is one of the checks which is designed for this job. More exactly: the 'software rollout' and 'last seen' situation of a certain client ist checked.

The result of the following checks is determined by two different states:

* The rollout state of one or more software products: +
The software rollout state results to: 
** 'OK' if the software is installed at the in the same product and package version which is aviable at the server and no action request is set.
** 'Warning' if the software is installed in version that is different to the servers version or if any action request is set.
** 'Critical' if there is a 'failed' reported by the last action.

* The time since 'last seen': +
The time since 'last seen' results to:
** 'OK' if the client has bee seen less or equal then 30 days before.
** 'Warning' if the client has bee seen more then 30 days before.


This check may used in different variants, here is the simplest one, which includes all software packages:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkClientStatus -c opsiclient.domain.local
----

As variant it is possible to exclude products from the check. For example:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkClientStatus -c opsiclient.domain.local -x firefox
----

In the example above the product firefox was excluded from the check. So this check would not switch to critical because the last action on firefox reported a failure.

[[opsi-Nagios-Connector-checks-opsiproductstatus]]
==== Check: opsi-check-ProductStatus

A other target of the opsi Nagios connector is the software rollout monitoring by viewing to single product or a group of products.

The result of the following checks is determined by the following states:

The software rollout state results to: 
* 'OK' if the software is installed at the in the same product and package version which is aviable at the server and no action request is set.
* 'Warning' if the software is installed in version that is different to the servers version or if any action request is set.
* 'Critical' if there is a 'failed' reported by the last action.


This checks has many variants and is so very flexible. The bast way to explain these variants are examples.

The simplest variant check one product on all clients. Here you hav to give the product as the opsi `productId`.

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox
----

In a simple one server opsi environment, this check is all you need to check the state of the product firefox on every client. +
You will get the information how many clients are in 'Warning' and in 'Critical' states. 

To get the information which clients exactly have the problems, you should call the check in the `verbose mode`:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox -v
----

A other variant is, that you may exclude a client from the check.

//// produkt muss angegebn werden ?! ////
[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox -x client.domain.local
----

In a opsi environment with multiple depot servers you have to use additional options to check also the clients that are not assigned to the configs servers depot. If you have multiple depots, you may give the depots name as parameter:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox -d depotserver.domain.local
----

The reason is, that the version of the software packages may differ between your depots. So every client has to be checked against the versions at the depot where they are assigned to. A advantage is that can place the display of the results to the depot server. +
You may give instead of the depot servers name the keyword 'all' which means all known depot servers. But this normally make only sense if you have only one or two depots. You may also give a comma separated list of depot servers.

A other way to define the checks is to give the name of a opsi groups. So you may check the software rollout state of all products in a given opsi product group. If you have for example a product group 'accounting' you may use the following call:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -g accounting
----

Now you will check all products that are Members of the opsi product group 'accounting' byt this single check. Important is to see, that the resolution of the opsi group is done while the check at the opsi server. So you may change the opsi group at the opsi Management interface and so you will change the products that will checked without any changes at the Nagios server.

NOTE: Sub groups (groups in groups) will not be resolved.

In the same way it is possible to define the clients that shold be checked by giving the name of a opsi client group. +
A example for a client group 'productiveclients':

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -g accounting -G productiveclients
----

This would check all products of the product group 'accounting' at all clients of the client group 'productiveclients'. 

NOTE: Sub groups (groups in groups) will not be resolved.

NOTE: You may also give a comma separated list of opsi groups

[[opsi-Nagios-Connector-checks-opsidepotsync]]
==== Check: opsi-check-depotsync 

If you are using multiple opsi depots the monitoring of synchronity is important. Even if yor depots are for good reasons not completly synchron, they should be synchron as much as possible to avoid problems by moving a client from one depot to another.

This check monitors if your depots are synchron according to product ids, product versions and package versions.

This check returns:

* 'OK' +
If all is in sync.
* 'Warning' +
If there is any difference

You should run this check always on the config server because all the data come from the backend of the config server.

Here are some examples.

The base variant:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus
----

This base variant is equivalent to the following call:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d all
----

So if you don't give the depots which are have to be checked, all known depots will be checked. If you have a lot of depots the interpratation of the result is complicated, so it is a good idea to define a lot of single checks where the depots are given as comma seperated list:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local
----

With this check you compare all products, that are installed *on both* depots. Any product which is installed only on one of the depot is ignored and will not effect the result.

If you want to include products which are not installed on all checked depots, you have to use the `strictmode` switch:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local --strictmode
----

Now also differences about missing products will be seen.

If you like to exclude a product from the check (perhaps because this product should be in different versions on different depots) you may do this by using the `-x` option. Here you may also use a comma seperated list:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local --strictmode -x firefox,thunderbird
----

This check will not warn if the products 'firefox' or 'thunderbird' or not in sync.

Instead of excluding products you may give a explicit list of products that has to been checked:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local --strictmode -e firefox,thunderbird
----

In this case *only* 'firefox' and 'thunderbird' will be checked. We recommend to use this check variant with `strictmode` to see if one of the products is missing.


[source,prompt]
----
./check_opsi.py -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSync
----

[[opsi-Nagios-Connector-checks-pluginOnClient]]
==== Check: nagios client plugin check via opsiclientd

This check gives you a easy possibility to integrate checks that collects the data direct on the client with a minimum of configuration work.

So this check tells the opsiclientd which is running at the opsi client to run a command, fetch the output and send it back.

This extension is not intended to be a complete replacement of a full featured Windows Nagios agent. It is only a light weight alternative.

The plugins which the opsiclientd may call must be compatible to the `Nagios plug-in development guidelines`. (More details at: http://nagiosplug.sourceforge.net/developer-guidelines.html ).

In order to run such a plugin on the client, it has to be installed at the client. This problem you will solve by deploying it as an opsi package. The path where the plugin is installed at client doen't matter because you have to give the complete path at check definition. We recommend to install all plugins in one directory to ease the maintenance of the plugins at the client.

For security reasons you should make sure that non privileged users have no write access to the plugins, because they will be executed from the opsiclientd with 'system' priviliges.

There are lot of ready to use plugins at the internet. One important address to look is: +
http://exchange.nagios.org/

In the following we assume that your plugins are installed at `C:\opsi.org\nagiosplugins\` and we will find ther the plugin `check_win_disk.exe` out of the package 'nagioscol' from +
http://sourceforge.net/projects/nagiosplugincol/

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkPluginOnClient --plugin "C:\opsi.org\nagiosplugincol\check_win_disk.exe C:" -c client.domain.local
----

This call checks the client `client.domain.local`. At the client the plugin `check_win_disk.exe` is called with the parameter `C:`. This means, that the hard drive with the letter 'C' should be checked. The output and the result value of the plugin will be fetched by the opsiclientd and will be given back to the Nagios server (via the opsi server) in a for Nagios correct format.

Another special feature is to hold the last check results, even if the client is not reachable.

This feature was implemented according to the fact that desktop clients not alwas are running like servers, but the most time in their life are usally switched off. Normally Nagios will show for switched off clients the result 'Unknown'. In fact the most problems on the monitored clients will not disapear by just switching them off and on again. So the information that a client had a problem before it was switched off may be a essential information for the system administrator. (You may try to solve this problem by using `Timeperiods` at the Nagios configuration, but we think that this is not flexible enough and leads to a permanent configuration work). So this opsi extension give you the possibility to give back the last real check results if the client is not reachable right now.

In order to use this feature, you have to use the Nagios macros `$SERVICESTATEID$` and `$SERVICEOUTPUT$`. `$SERVICESTATEID$` gives the last result value and should be passed to the `-s` Option. `$SERVICEOUTPUT$` gives the last output line and should be passed to the `-o` Option. So check can give these last values instead of 'Unknown' if the client is not reachable.


[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkPluginOnClient --plugin "C:\opsi.org\nagiosplugincol\check_win_disk.exe C:" -c client.domain.local -s $SERVICESTATEID$ -o  $SERVICEOUTPUT$
----

[[opsi-Nagios-Connector-configuration]]
== opsi monitoring configuration

This chapte focusses on the configuration that have to been made for a working interface between the opsi and the Nagios server. Just see this as a recommendation, there will be  a lot of other ways to do the job.

This description uses a Nagios server as monitoring server. On a Icinga server it should work very similar but you have to change some path entries. It should also work on other Nagios derivates, but this is not tested.

[[opsi-Nagios-Connector-configuration-User]]
=== opsi monitoring user

In monitoring environments you will often find that the access is just resticted by IP numbers. Because of the lack of security of this solution we decided to work with a real user / password security in this opsi extension.

Using the opsi standard group `opsiadmin` would give the Nagios more rights tha needed. So you have to create a own opsi user for the opsi nagios connector.

In the following example a user named 'monitoring' with the password 'monitoring123' is created for opsi:

[source, prompt]
----
opsi-admin -d method user_setCredentials monitoring monitoring123
----

The created user 'monitoring' will be stored with its encrypted password at the `/etc/opsi/passwd` and is not a user which may be used to login at a shell. In fact it is no real unix user.

You have to create this user only on your config server, even if you have multiple depots.

At your Nagios server you should mask the user and password by making an entry at the `/etc/nagios3/ressource.cfg`. This should look for example like this:

[source, prompt]
----
$USER2$=monitoring
$USER3$=monitoring123
----

The number behind '$USER' may vary. If this configuration was not used before, there should be only `$USER1$` be used. According to what you are using here, you might have to change the other examples in this manual.

[[opsi-Nagios-Connector-configuration-template]]
=== Nagios templates

There are a lot of templates for different objects at Nagios. This is a standard at Nagios which is not described here. But we are using this functionality to make the configuration for opsi much easier.



Da die meisten Checks auf den Configserver ausgeführt werden, sollte man sich als erstes ein Template für die opsi-Server und ein Template für die opsi-Clients schreiben. Da es in einer Multidepot-Umgebung nur einen Configserver geben kann, kann man direkt diesen ins Template mit übernehmen. Dies erreicht man am einfachsten über eine Custom-Variable. Diese erkennt man daran, dass Sie mit einem `_` beginnen. Es wird in das Template für den opsi-Server und für die opsi-Clients folgendes zusätzliche in das Template eingetragen:

[source,prompt]
----
_configserver           configserver.domain.local
_configserverurl        4447
----

Auf diese beiden `custum Variablen` kann man später einfach mit dem Nagios-Makro: `$_HOSTCONFIGSERVER$` und `$_HOSTCONFIGSERVERPORT$`, verweisen. HOST muss vorangeführt werden, da diese Custum-Variablen in einer Hostdefinition vorgenommen wurden. Weitere Informationen zu Custom-Variablen entnehmen Sie bitte der Nagios-Dokumentation. Da diese beiden Konfigurationen im Template vorgenommen werden müssen, gelten Sie für jede Hostdefinition, die später von diesen Templates erben. Aber dazu später mehr.

Um nun die Templates an zu legen sollte man unterhalb von: `/etc/nagios3/conf.d` als erstes ein Verzeichnis `opsi` erstellen. Dies macht die Konfiguration später Übersichtlicher, da alle Konfigurationen die den opsi-Nagios-Connector betreffen gebündelt auf dem Server abgelegt sind. In diesem Verzeichnis erstellt man die Datei und benennt Sie auch direkt so, dass man später erkennt, was genau hier Konfiguriert werden soll: `opsitemplates.cfg`. Nun kann man diese Datei noch mit Inhalt füllen. Zunächst die Opsi-Server:

[source,ini]
----

----

die Standardmäßig geplant sind. Es gibt ein paar Templates, die dennoch erstellt werden sollten, diese werden hier kurz erläutert und sollen einen Einblick dazu geben, zur Besseren Verständlichkeit wurden die Kommentare zu den einzelnen Einstellungen nicht gelöscht.

Die folgenden Einstellungen sind in der Regel in der Datei /etc/nagios3/templates.cfg zu erledigen.

Zunächst sollte ein neues Template-Objekt für die Opsi-Clients erstellt werden:

[source, prompt]
----
#OPSI-Clients
define host{
        name                            opsiclient    ; The name of this host template
        notifications_enabled           1       ; Host notifications are enabled
        event_handler_enabled           1       ; Host event handler is enabled
        flap_detection_enabled          1       ; Flap detection is enabled
        failure_prediction_enabled      1       ; Failure prediction is enabled
        process_perf_data               0       ; Process performance data
        retain_status_information       1       ; Retain status information across program restarts
        retain_nonstatus_information    1       ; Retain non-status information across program restarts
                max_check_attempts              10
                notification_interval           0
                notification_period             24x7
                notification_options            d,u,r
                contact_groups                  admins
        register                        0       ; DONT REGISTER THIS DEFINITION - ITS NOT A REAL HOST, JUST A TEMPLATE!
        icon_image                      opsi/opsi-client.png
        }
----

Hauptsächlich sollte dieses Template erstellt werden aus folgenden Gründen:

* Da die Clients in der Regel nicht durchlaufen, sollte die Option: "check command	check-host-alive" nicht gesetzt wird. Somit werden die Clients als Pending angezeigt und nicht als Offline.
* Optional kann durch die Option icon_image ein Image gesetzt werden, dieser muss relativ zum Pfad: /usr/share/nagios3/htdocs/images/logos/ angegeben werden.
* Optional kann auch eine eigene contact_group angegeben werden, die allerdings als Contact-Object in der contacts.cfg angelegt sein muss.

Nach dem selben Prinzip kann auch ein Template für opsi-Server erstellt werden. Auf ein weiteres Template-Beispiel wird verzichtet, da die Templates sich sehr ähneln.

Wenn pnp4nagios für die grafische Darstellung von den Performancedaten für den Opsi-Webservice eingesetzt werden, sollten noch folgende zwei Objekte als Template angleget werden:

[source,prompt]
----
define host {
   name       host-pnp
   action_url /pnp4nagios/index.php/graph?host=$HOSTNAME$&srv=_HOST_
   register   0
}

define service {
   name       srv-pnp
   action_url /pnp4nagios/index.php/graph?host=$HOSTNAME$&srv=$SERVICEDESC$
   register   0
}
----

[[opsi-Nagios-Connector-configuration-hostobjects]]
=== opsi-Server und opsi-Clients

Um die Übersichtlichkeit zu wahren, sollte unterhalb von `/etc/nagios3/conf.d` ein Verzeichnis opsi erstellt werden. Somit bleiben die opsi-Infrastruktur betreffenden Konfigurationen an einer Stelle gekapselt.

Als nächstes sollten folgende Hostgruppen erstellt werden. Dies dient zum einen der besseren Übersicht auf der Weboberfläche und hiltf beim konfigurieren der Services. Dafür sollte eine Datei Namens opsihostgroups.cfg mit folgendem Inhalt erstellt werden:

[source,prompt]
----
define hostgroup {
        hostgroup_name  opsi-clients
        alias           OPSI-Clients
}

define hostgroup {
        hostgroup_name  opsi-server
        alias           OPSI-Server
}
----

Als nächstes sollten die opsi-Server konfiguriert werden. Dies kann jeweils in einer eigenen Datei wie zum Beispiel configserver.domain.local.cfg oder eine Datei mit allen opsi-Host wie opsihost.cfg. Der Inhalt sollte für opsiserver folgendermaßen aussehen:

[source,prompt]
----
define host{
        use				opsiserver
        host_name		configserver
        hostgroups		opsi-server
        alias				opsi Configserver
        address			configserver.domain.local
        }

define host{
        use				opsiserver
        host_name		depotserver
        hostgroups		opsi-server
        alias				opsi Depotserver
        address			depotserver.domain.local
        }
----

Folgende Erläuterungen zu den Werte:
* use bezeichnet welches Template benutzt wird.
* hostgroups gibt an, welcher Hostgruppe der Server angehört.

Die opsi-Clients sollten mindestens folgendermaßen definiert werden:

[source,prompt]
----
define host{
        use				opsiclient
        host_name		client.domain.local
        hostgroups		opsi-clients
        alias				opsi client
        address			client.domain.local
        _depot_id			configserver.domain.local
        }
----

Die Clientkonfiguration bedient sich einer Sonderfunktion von Nagios. Die Option `_depot_id`
ist eine sogenannte Benutzerdefinierte Variable, die als Makro `$_depot_id$` benutzt werden kann. Dies ist nötig da die meisten Checks über den Configserver ausgeführt werden.

[[opsi-Nagios-Connector-configuration-commands]]
=== opsi Check-Kommandos Konfiguration

Die oben beschriebenen Check-Kommandos müssen nun mit den bis bisherigen Kommandos konfiguriert werden. Dafür solle eine Datei mit dem Namen opsicheckcommands.cfg erstellt werden. Hier wird eine Bespielkonfiguration angegeben, je nach dem welche Funktionen Sie auf welche Weise verwenden wollen, müssen Sie diese Einstellungen nach eigenem Ermessen modizieren.

Als erstes wird der Aufbau Anhand eines Beispiels erläutert:

[source,prompt]
----
define command{
        command_name	check_opsi_clientstatus
        command_line		$USER1$/check_opsi.py -H $_HOSTDEPOT_ID$ -u $USER2$ -p $USER3$ -P 4447 -t checkClientStatus -c $HOSTADDRESS$
        }
----

Der `command_name` dient zur Referenzierung der weiteren Konfiguration. In der Option `command_line` werden die Konfigurationen  und das Check-Kommando als erstes vereint.

Nach diesem Prinzip baut man nun die ganze Datei auf:

[source,prompt]
----
define command{
        command_name    check_opsi_clientstatus
        command_line    $USER1$/check_opsi.py -H $_HOSTDEPOT_ID$ -p 4447 -t checkClientStatus -c $HOSTADDRESS$
        }
define command{
        command_name    check_opsi_productStatus
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -P $ARG1$ -t checkProductStatus
        }
define command{
        command_name    check_opsi_productStatus_detailed
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -P $ARG1$ -t checkProductStatus -v
        }
define command{
        command_name    check_opsi_productStatus_group
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -g $ARG1$ -t checkProductStatus -v -d $HOSTADDRESS$
        }
define command{
        command_name    check_opsi_pluginonclient
        command_line    $USER1$/check_opsi.py -H $_HOSTDEPOT_ID$ -p 4447 -t checkPluginOnClient -c $HOSTADDRESS$ --plugin $ARG1$
}
define command{
        command_name    check_opsi_pluginonclient_withState
        command_line    $USER1$/check_opsi.py -H $_HOSTDEPOT_ID$ -p 4447 -t checkPluginOnClient -c $HOSTADDRESS$ --plugin $ARG1$ -s $RG2$ -o $ARG3$
}

define command {
        command_name    check_opsiwebservice
        command_line    $USER1$/check_opsi_webservice.py -H $HOSTADDRESS
}

define command {
        command_name    check_opsi_depotsync
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -t checkDepotSyncStatus -d $ARG1$
}

define command {
        command_name    check_opsi_depotsync_withexclude
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -t checkDepotSyncStatus -d $ARG1$ -x $ARG2$
}
----


