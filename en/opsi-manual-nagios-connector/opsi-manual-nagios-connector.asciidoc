////
; Copyright (c) uib gmbh (www.uib.de)
; This documentation is owned by uib
; Until we found a better license:
; All rights reserved.
; credits: http://www.opsi.org/credits/
////

:Author:    uib gmbh
:Email:     info@uib.de
:Date:      13.01.2012
:Revision:  4.0
:toclevels: 6


include::../common/opsi_terms.asciidoc[]

= opsi-Nagios-Connector

[[opsi-Nagios-Connector-introduction]]
== Introduction

Beside client managment is monitoring one the central functions in a modern IT service management. With opsi you got a client management tool. For monitoring tasks there are other well known open source solutions. So we build for the monitoring tasks in opsi not a own monitoring tool but a interface to existing solutions. With this opsi extension we provide a connector to Nagios. +
In the following chapters the design and configuration of the opsi-nagios-connector is explained.

The opsi-nagios-Connector isn't stricly bound to Nagios. It is developed vor the use with Nagios and Icinga. It should also work with other Nagios derivates but this is wether tested nor supported.

The scope of this manual is the design and configuration of the opsi-Nagios-Connector. It is not a Nagios manual. You will need a running Nagios installation and the knowlege how to run Nagios.

[[opsi-Nagios-Connector-prerequires]]
== Preconditions

[[opsi-Nagios-Connector-prerequires-opsi]]
=== Preconditions at the opsi server and client

This extension is at the moment a co-funding project which means that until the complete development costs are payed by co-funders, they are only allowed to use by the co-funders or for evaluation purposes. If we have earned the development cost we will give these modules for everybody for free. +
see also +
http://uib.de/en/opsi_cofunding/index.html
http://www.opsi.org/en/statistics

So as precondition to use this extension you need as first an activation file.
For evaluation purpose you will get a temporary activation file if you ask for it in a mail at info@uib.de.

Technical preconditions are opsi 4.0.1 with the following package and product versions:

.Needed product and package versions
[options="header"]
|==========================
|opsi-Paket|Version
|opsi-client-agent|>=4.0.1-26
|opsiconfd|>=4.0.1.11-1
|python-opsi|>=4.0.1.38-1
|==========================

[[opsi-Nagios-Connector-prerequires-nagios]]
=== Preconditions at the Nagios server


As precondition you need a Nagios installation in the version 3.x or a Icinga Installation in the version 1.6 or higher. +
For graphical output of performance data a pnp4nagios installation is required.

Further information you will find at:

www.nagios.org

www.icinga.org

www.pnp4nagios.org

[[opsi-Nagios-Connector-concept]]
== Concept

The opsi-Nagios-Connector contains of two core components. At first we will discuss these core components.

[[opsi-Nagios-Connector-concept-webservice]]
=== opsi web service extension

The heart of the opsi-Nagios-Connector are extened features of the opsi web service. These web service extension make it possible to run checks via web service on the opsi server. So the nagios server calls checks via web service, which are executed on the opsi-server and the results come back to the nagios server via opsi web service. Tha advantage of this solution is that there is nearly nothing to do on the monitored opsi server.

The focus of the opsi web service extension lies on opsi specific checks like e.g. rollout monitoring. For the 'normal' server monitoring you should use still standard check methods. 

[[opsi-Nagios-Connector-concept-opsiclientd]]
=== opsi-client-agent extension

A other part of the opsi-Nagios-Connector is a extension of the opsi-client-agent. +
In a opsi environment on every managed client runs a opsi-client-agent. With this extension you may use the opsi-client-agent as nagios agent as well. But in fact not all features of a standard Nagios agent like `NSClient++` are implemented at the opsi-client-agent. You may use the opsi-client-agent to run command line programs and send back the output.
efern der Ergebnisse.
////
Wenn man nicht alle Funktionen, wie NSCA benötigt, sondern nur ein paar Standard-Checks per Plugin auf den Clients ausführen oder eine Reihe von eigenen Plugins auf den Clients benutzen möchte, kann man den opsi-client-agent dazu verwenden. 
////

If you need more features for the client monitoring you should rollout a standard agent like `NSClient++` via opsi.

The advantage of using the opsi-client-agent as nagios agent is, that you don't need a additional agent on the client and that you don't need any access data for the clients at the monitoring server. These data is not needed because all check run via the opsi server. This makes the configuration a lot more easier.

[[opsi-Nagios-Connector-checks]]
== opsi-checks

The following chapter explains the goals and configurations of the opsi-checks.

[[opsi-Nagios-Connector-checks-background]]
=== Some background information about where to run the checks

Monitoring administrators know the difference between active and passive checks. 

With the opsi-Nagios-Connector we get a new difference: direct and indirect.

* direct : +
The check which collects information about a client runs on that client, get the information direct from the client and sends the information back.

* indirect : +
The check which collects information about a client runs on the opsi server and get the information from the opsi configuration data which is stored in the opsi backend. So - this information may be different from the actual situation of the client.

A good example for a indirect check is the `check-opsi-client-status`. This check gives you for a given client information about pendig action request and reported failures of the opsi software depolyment. So this are information about the client from the opsi servers point of view. Therefore this check runs on the opsi server and is a indirect check. A check which runs on the client is a direct check.

For a correct distribution and configuration of the checks you have to analyze your opsi installation. +
Acoording to the flexibility of opsi many various opsi configurations are possible. So here we can only explain some typical situations. Of course we will get help for special sitations by our comercial support.

only one opsi server: +
The opsi stand alone installation is the sitation that you will find at the most opsi environments. At this installation the opsi config server functionality is at the same server like the (one and only) opsi depot server functionality. +
This means to you, that you may ignore if a check has to be run on the config server or the depot server.

.Scheme of a standalone opsi server
image::../images/opsi-config-server.png["opsi config server",width=200]

opsi with multiple depotservers: +
If you have a central management of a multi location opsi environment (one config server, multiple depot servers) the sitation is more complicated. So you have to understand the sitation:

.Scheme opsi multi depot environment
image::../images/central-config-server.png["opsi multi depot environment",width=200]

As the figure points out there is only one server which have data storage for the configuration data - the data backend. This is the opsi config server. The opsi depot server has no own data storage but a redireted backend. This means that if you ask a depot server for any configuration data, this question will be redireted to the config server. And this leads to the consequence that every check which runs against the opsi data backend will at least run on the config server. So you should address checks that run against the backend always to the config server. Even in the sitation if you are collecting information about clients which are assingned to a depot which is different from the config server and the check is logically part of the check of this depot server.

If you running direct checks you normally also address the config server. You may address the depot server if the clients can't be reached by the opsi config server via port 4441. In this case it is a good idea to address the depot server.

VERTEILTE Checks Bild

[[opsi-Nagios-Connector-checks-plugin]]
=== opsi-check-plugin

At the Nagiso server there is only one opsi-check-plugin which provides a wide range of different checks. According to the number of features there is also a big number of command lin options. So - just list all these options won't help you much. Instead the option will be explained in the context of documentation of the possible checks. +
How ever to get a listing of all options you may call `check_opsi` with the parameters `--help` or `-h`.

The following general options are needed for every check:

.General Options
|=======================
|Option|Description|Example
|-H,--host|opsi server which should run the check|configserver.domain.local
|-P,--port|opsi webservice port|4447 (Default)
|-u,--username|opsi monitoring user|monitoring
|-p,--password|opsi monitoring password|monitoring123
|-t,--task|opsi check method (case sensitive)|
|=======================

The following chapter describes how to call the opsi-check-plugin is called on the command line. How you have to configure these calls at your Nagios server is described at the chapter 'configuration'.

In order to install the opsi-check-plugin on your Nagios server you should add the opsi repository to your server and install the package 'opsi-nagios-plugins'. +
For example at Debian or Ubuntu with the following commands:

[source,prompt]
----
apt-get install opsi-nagios-plugins
----

At the moment this package is only aviable for Debian/Ubuntu. Packages for other distribution will follow later. Nevertheless the plugin it self is written in python and should ran at any distribution.

The package bases on the package 'nagios-plugins-basic' and installs the plugin to `/usr/lib/nagios/plugins`. +
According to the flexibility of the check_plugin there is no automatic configuration.

[[opsi-Nagios-Connector-checks-opsiWebservice]]
==== Check: opsi web service 

This check monitors the opsi web service process (opsiconfd). This check returns also performance data. You should run this check on every opsi server because every opsi server have a opsiconfd process.

[source,prompt]
----
check_opsi.py -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiWebservice
----

This check return normally OK. +
You will get other return values in the following situations:

* Critical: +
** If the opsiconfd is in trouble and can't reply correctly.
** If the opsiconfd consumes more than 80% of the cpu.
** If you have a rate of RPC errors of more than 20%.

* Warning: +
** If the opsiconfd consumes more than 60% (but less than 80%) of the cpu.
** If you have a rate of RPC errors of more than 10% but less than 20%

* Unknown: +
The opsi web service could not be reached.

NOTICE:
The percentage value of the cpu consumption belongs always to one cpu because the opsiconfd only may use one cpu. (This may change with the opsi multi processing extension)

[[opsi-Nagios-Connector-checks-opsiWebservice-pnp4nagios-template]]
===== Check: opsi web service pnp4nagios template

For the display of performance data there is a template for pnp4nagios which displays the data in a combined way. +
Here is not described how to install pnp4nagios. We assume that pnp4nagios is installed and configured correctly. The way you have to use to configure our template may differ from the below described way according to your pnp4nagios installation (which may use different path).

Standard templates dispay for every performance data a own diagram. To create a combined display you have to go the following steps:

Step 1: +
create at `/etc/pnp4nagios/check_commands` a file named  `check_opsiwebservice.cfg` and insert the following content:

[source,cmd]
----
CUSTOM_TEMPLATE = 0
DATATYPE = ABSOLUTE,ABSOLUTE,ABSOLUTE,ABSOLUTE,DERIVE,GAUGE,GAUGE,GAUGE
----

Setp 2: +
change to the directory `/usr/share/pnp4nagios/html/templates` and place there a file `check_opsiwebservice.php` which you check out from svn.opsi.org:

[source,cmd]
----
cd /usr/share/pnp4nagios/html/templates
svn co https://svn.opsi.org/opsi-pnp4nagios-template/trunk/check_opsiwebservice.php
----

Please check that your php file is named exactly like the 'command_name' which is defined at the `/etc/nagios3/conf.d/opsi/opsicommands.cfg`. If the names don't match, a standard template will be used instead our combined template.

After installing this template you should delete the RRD data bases which belong to this check (if there any existing). You will find these data bases at `/var/pnp4nagios/perfdata/<host>/` where you should (only) delete the `opsi-webservice.rrd` and `opsi-webservice.xml` files.

If you have configured everything correctly you should now able to see diagrams like the following screenshot.


image::../images/pnp4nagios.png["uib template for pnp4nagios",width=400]

[[opsi-Nagios-Connector-checks-opsidiskusage]]
==== Check: opsi-check-diskusage

This check monitors the usage of the resources (directories) which are used by opsi. The following table shows the resource names and the corrosponding directories:

.opsi Ressourcen
|=======================
|Ressource name|Path
|/|/usr/share/opsiconfd/static
|configed|/usr/lib/configed
|depot|/opt/pcbin/install
|repository|/var/lib/opsi/repository
|=======================

Please note that this check monitors only opsi relevant data and do replace a general disk usage check for the server.

The following command retrieves all resources at one time:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDiskUsage
----

In addition to this standard variant you may restrict the check to the resource `depot`:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDiskUsage -r depot
----

The default result valu of this check is 'OK' and the free space of the resources. The free space is given in Gigabyte. The default values for the 'Warning' and 'Critical' results are:

* WARNING: If at least one resource have 5GB or less free space.
* CRITICAL: If at least one resource have 1GB or less free space.

This are the default tresholds. They may changed by giving other values for 'Warning' with the -W or --warning optionen and for 'Critical' wit the -C or --critical option. With these options you can give the tresholds as Gigabyte (G) and as percent (%) as well. The prodused output uses the same unit which is used to define the tresholds. +
Finally a example:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDiskUsage -r depot --warning 10% --critical 5%
----

[[opsi-Nagios-Connector-checks-clientstatuscheck]]
==== Check: opsi-client-status 

One of the targets of the opsi Nagios connector is the software rollout monitoring by viewing to single clients. This is one of the checks which is designed for this job. More exactly: the 'software rollout' and 'last seen' situation of a certain client ist checked.

The result of the following checks is determined by two different states:

* The rollout state of one or more software products: +
The software rollout state results to: 
** 'OK' if the software is installed at the in the same product and package version which is aviable at the server and no action request is set.
** 'Warning' if the software is installed in version that is different to the servers version or if any action request is set.
** 'Critical' if there is a 'failed' reported by the last action.

* The time since 'last seen': +
The time since 'last seen' results to:
** 'OK' if the client has bee seen less or equal then 30 days before.
** 'Warning' if the client has bee seen more then 30 days before.


This check may used in different variants, here is the simplest one, which includes all software packages:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkClientStatus -c opsiclient.domain.local
----

As variant it is possible to exclude products from the check. For example:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkClientStatus -c opsiclient.domain.local -x firefox
----

In the example above the product firefox was excluded from the check. So this check would not switch to critical because the last action on firefox reported a failure.

[[opsi-Nagios-Connector-checks-opsiproductstatus]]
==== Check: opsi-check-ProductStatus

A other target of the opsi Nagios connector is the software rollout monitoring by viewing to single product or a group of products.

The result of the following checks is determined by the following states:

The software rollout state results to: 
* 'OK' if the software is installed at the in the same product and package version which is aviable at the server and no action request is set.
* 'Warning' if the software is installed in version that is different to the servers version or if any action request is set.
* 'Critical' if there is a 'failed' reported by the last action.


This checks has many variants and is so very flexible. The bast way to explain these variants are examples.

The simplest variant check one product on all clients. Here you hav to give the product as the opsi `productId`.

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox
----

In a simple one server opsi environment, this check is all you need to check the state of the product firefox on every client. +
You will get the information how many clients are in 'Warning' and in 'Critical' states. 

To get the information which clients exactly have the problems, you should call the check in the `verbose mode`:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox -v
----

A other variant is, that you may exclude a client from the check.

//// produkt muss angegebn werden ?! ////
[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox -x client.domain.local
----

In a opsi environment with multiple depot servers you have to use additional options to check also the clients that are not assigned to the configs servers depot. If you have multiple depots, you may give the depots name as parameter:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -e firefox -d depotserver.domain.local
----

The reason is, that the version of the software packages may differ between your depots. So every client has to be checked against the versions at the depot where they are assigned to. A advantage is that can place the display of the results to the depot server. +
You may give instead of the depot servers name the keyword 'all' which means all known depot servers. But this normally make only sense if you have only one or two depots. You may also give a comma separated list of depot servers.

A other way to define the checks is to give the name of a opsi groups. So you may check the software rollout state of all products in a given opsi product group. If you have for example a product group 'accounting' you may use the following call:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -g accounting
----

Now you will check all products that are Members of the opsi product group 'accounting' byt this single check. Important is to see, that the resolution of the opsi group is done while the check at the opsi server. So you may change the opsi group at the opsi Management interface and so you will change the products that will checked without any changes at the Nagios server.

NOTE: Sub groups (groups in groups) will not be resolved.

In the same way it is possible to define the clients that shold be checked by giving the name of a opsi client group. +
A example for a client group 'productiveclients':

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkProductStatus -g accounting -G productiveclients
----

This would check all products of the product group 'accounting' at all clients of the client group 'productiveclients'. 

NOTE: Sub groups (groups in groups) will not be resolved.

NOTE: You may also give a comma separated list of opsi groups

[[opsi-Nagios-Connector-checks-opsidepotsync]]
==== Check: opsi-check-Depotsync 

Gerade in einer Mutlidepotumgebung ist es wichtig, die Depotserver auf Synchronität zu überwachen. Entscheidend ist bei diesem Check die Software- und Paketversion der installierten Produkte. Manchmal ist eine differentierter Einsatz der opsi-Produkte auf Depotservern gewünscht, birgt aber die Gefahr, dass bei einem Umzug von einem Client, von einem Depot zum anderen, inkonsistenzen in der Datenbank enstehen können. Um dieser Problematik entgegen zu wirken, wird empfohlen die opsi-Pakete auf den Depotservern so synchron wie möglich zu halten.

Standardmäßig liefert dieser Check OK zurück, sollte eine Differenz festgestellt werden, wird der Status: WARNING zurückgegeben. Dieser Check ist ein klassischer Check, der auf dem Configserver ausgeführt werden sollte, da alle Informationen zu diesem Check nur im Backend auf dem opsi-Configserver zu finden sind.

Als nächstes folgen ein paar Anwendungsmöglichkeiten dieses Checks:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus
----

Dies ist dies Basis-Variante und äquivalent zu folgendem Aufruf:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d all
----

Ohne konkrete Angabe von Depotserver werden die Produkt-Listen aller Depot-Server miteinander verglichen. Um eine bessere Übersichtlichkeit zu schaffen, sollte man diesen Check auf zwei Depotserver reduzieren und lieber auf mehrere Checks verteilen. Dies erreicht man durch direkte Angabe der Depotserver:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local
----

Mit diesem Aufruf werden alle Produkte verglichen, die auf beiden Depotservern installiert sind. Sollte ein Produkt auf einem Depotserver gar nicht installiert sein, hat dies keine Auswerkungen auf das Check-Resultat. DIes kann man ändern, indem man diesem Check den "strictmode" Schalter setzt:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local --strictmode
----

Nun werden auch Produkte angezeigt, die auf einem Depotserver nicht installiert sind. Um ein bestimmtes Produkt oder bestimmte Produkte nicht mit zu checken, weil man zum Beispiel will dass diese Produkte in verschiedenen Versionen eingesetzt werden, kann man diese Produkte von diesem Check ausschliessen:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local --strictmode -x firefox,thunderbird
----

Dieser Check würde auch dann ein OK zurückgeben, wenn das firefox und das thunderbird-Paket nicht überall Synchron eingesetzt werden.

Ein weitere Einsatzmöglichkeit ist, dass nur eine Auswahl von Produkten auf Synchronität überwacht werden können. Dies kann man durch:

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSyncStatus -d configserver.domain.local,depotserver.domain.local --strictmode -e firefox,thunderbird
----

So werden nur firefox und thunderbird Produkte auf Synchronität überwacht. Bei diesem Check sollte der `strictmode` gesetzt sein, damit man auch erkennt, wenn die gewünschten Produkte auf Depotservern nicht installiert sind.


[source,prompt]
----
./check_opsi.py -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkOpsiDepotSync
----

[[opsi-Nagios-Connector-checks-pluginOnClient]]
==== Check: Pugin über OpsiClientd checken

Dieser Check führt ein Check-Plugin auf dem Client direkt aus und fängt die Ausgabe ein. 

Diese Erweiterung soll keinen Ersatz für einen richtigen Nagiosagent bieten, sonderne eine alternative bieten. Man kann diese Erweiterung Einsetzen, wenn man Plugins auf dem opsi-Client checken will. Die eingesetzten Plugins müssen der sogenannten: `Nagios plug-in development guidelines` entsprechen. (Weitere Infos unter: http://nagiosplug.sourceforge.net/developer-guidelines.html)

Um ein Plugin ausführen zu können, muss man das Plugin erst einmal auf den Clients verteilen. Dies sollte man über ein opsi-Paket lösen. Der Ablageort für die Plugins auf dem Client ist im ersten momentan egal, da man den Pfad beim Checken mit angeben muss. Allerdings sollte man die Plugins nicht einzeln verteilen, sondern in einem Verzeichnis zusammenführen, damit das Aktualisieren und Pflegen der Plugins einfacher wird. Weiterhin sollte man auch Sicherheitstechnisch im Hinterkopf behalten, dass die Plugins im Systemkontext des opsiclientd-Services aufgerufen werden. Normale Anwender sollten auf dieses Verzeichnis keinen Zugang haben.

Es gibt Diverse Plugins, die es schon vorgefertigt im Internet runter zu laden gibt. Eine mögliche Anlaufstelle ist (http://exchange.nagios.org/)

Im folgenden wird davon ausgegangen, dass unter C:\opsi.org\nagiosplugins\ das Plugin check_win_disk.exe vom Paket nagioscol (http://sourceforge.net/projects/nagiosplugincol/) abgelegt wurde.

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkPluginOnClient --plugin "C:\opsi.org\nagiosplugincol\check_win_disk.exe C:" -c client.domain.local
----

Dieser Aufruf checkt auf dem Client `client.domain.local` das Plugin check_win_disk.exe und übergibt diesem den Parameter `C:`. Dies bedeutet, dass das Laufwerk C auf dem Client gecheckt wird. Die Ausgabe und der Rückgabewert dieses Plugins, wird direkt richtig ausgewertet und für Nagios verarbeitbar weitergereicht.

Ein besonderes Feature ist das beibehalten von Zuständen. Diese Implementation ist aus der Problemstellung entstanden, dass Clients nicht wie Server durchlaufen, sondern in der Regel nur einen bestimmten Zeitraum eingeschaltet sind. Man kann den Check auf Nagios-Seite zwar mit sogenannten `Timeperiods` eingrenzen, aber in der Praxis ist so ein vorgehen nicht praktikabel, da man zum Beispiel auch bei Urlaub von Anwendern flexibel reagieren müsste. Dies würde eine ständige Konfigurationsarbeit nach sich ziehen. Wenn man darauf verzichtet, wird der Status ständig geändert, auch wenn ein aufgetretenes Problem noch gar nicht gelöst ist. Deshalb kann man den letzten bekannten Status an opsi Übergeben. Sollte der Client nicht erreichbar sein, wird dieser letzte bekannte Status zurückgegeben. Ein Critical-Zustand zum Beispiel, bleibt in diesem Falle auch auf Critical stehen und wechselt nicht auf Unknown, was rein logisch aber richtig wäre.

Um dieses Feature später mit Nagios zu verwenden kann man die Nagios-Makros: $SERVICESTATEID$ und $SERVICEOUTPUT$ verwenden.

[source,prompt]
----
check_opsi -H configserver.domain.local -P 4447 -u monitoring -p monitoring123 -t checkPluginOnClient --plugin "C:\opsi.org\nagiosplugincol\check_win_disk.exe C:" -c client.domain.local -s $SERVICESTATEID$ -o  $SERVICEOUTPUT$
----

[[opsi-Nagios-Connector-configuration]]
== opsi Monitoring Konfiguration

Dieses Kapitel widmet sich der Konfiguration der Schnittstelle von opsi und dem Nagios-Server. Die Konfigurationen in diesem Kapitel, besonders die auf dem Nagios-Server sollen als Empfehlungen gelten sind aber nicht die einzigen Lösungen. 
Hier wird nur die Konfiguration mit einem Nagios-Server beschrieben. Mit einem Icinga-Server sollte, mit Ausnahme von ein paar Pfaden, die Konfiguration ziemlich genauso funktionieren. Andere Derivate auf Nagios Basis sollten funktionieren, wurden aber nicht getestet.

[[opsi-Nagios-Connector-configuration-User]]
=== opsi Monitoring User

In der Regel wird im Monitoring-Bereich viel mit IP-Freischaltungen als Sicherheit gearbeitet. Da aber dieser Mechanismus nicht wirklich einen Schutz bietet, wurde beim opsi-Nagios-Connector darauf verzichtet. Aus diesem Grund wird das ganze per Benutzer und Passwort geschützt. Diesen User als opsi-admin einzurichten, würde aber auch hier zu viele Rechte freischalten, da dieser User nur für diese Schnittstelle von nöten ist und auch die Benutzbarkeit auf diesen Bereich eingeschränkt werden soll, wird der User nur intern in opsi eingerichtet. Folgender Befehl legt den User an:

[source, prompt]
----
opsi-admin -d method user_setCredentials monitoring monitoring123
----

Dieser Befehl legt den User: monitoring mit dem Passwort monitoring123 an. Der User wird in der /etc/opsi/passwd angelegt und ist auch kein User, mit dem man sich an der Shell anmelden könnte. 

Bei einer Multidepot-Umgebung muss man diesen User nur auf dem Configserver ereugen.

Beim nagios-Server kann man dieses Passwort vor den CGI-Skripten maskieren, indem man einen Eintrag in der /etc/nagios3/ressource.cfg vornimmt. Dieser sieht zum Beispiel so aus:

[source, prompt]
----
$USER2$=monitoring
$USER3$=monitoring123
----


Die Zahl hinter USER kann varieren. Wenn diese Datei vorher nicht genutzt wurde, sollte in der Regel nur das $USER1$ belegt sein. Diese Konfiguration dient als Grundlage für die weiteren Konfigurationen.

[[opsi-Nagios-Connector-configuration-template]]
=== Nagios Templates

Es gibt diverse Templates für diverse Objekte im Nagios. Dies ist eine Standard-Nagios Funktionalität, die hier nicht näher beschrieben wird. Diese Funktionalität kann man sich für opsi zu nutze machen, um sich später bei der Konfiguration die Arbeit zu vereinfachen.

Da die meisten Checks auf den Configserver ausgeführt werden, sollte man sich als erstes ein Template für die opsi-Server und ein Template für die opsi-Clients schreiben. Da es in einer Multidepot-Umgebung nur einen Configserver geben kann, kann man direkt diesen ins Template mit übernehmen. Dies erreicht man am einfachsten über eine Custom-Variable. Diese erkennt man daran, dass Sie mit einem `_` beginnen. Es wird in das Template für den opsi-Server und für die opsi-Clients folgendes zusätzliche in das Template eingetragen:

[source,prompt]
----
_configserver           configserver.domain.local
_configserverurl        4447
----

Auf diese beiden `custum Variablen` kann man später einfach mit dem Nagios-Makro: `$_HOSTCONFIGSERVER$` und `$_HOSTCONFIGSERVERPORT$`, verweisen. HOST muss vorangeführt werden, da diese Custum-Variablen in einer Hostdefinition vorgenommen wurden. Weitere Informationen zu Custom-Variablen entnehmen Sie bitte der Nagios-Dokumentation. Da diese beiden Konfigurationen im Template vorgenommen werden müssen, gelten Sie für jede Hostdefinition, die später von diesen Templates erben. Aber dazu später mehr.

Um nun die Templates an zu legen sollte man unterhalb von: `/etc/nagios3/conf.d` als erstes ein Verzeichnis `opsi` erstellen. Dies macht die Konfiguration später Übersichtlicher, da alle Konfigurationen die den opsi-Nagios-Connector betreffen gebündelt auf dem Server abgelegt sind. In diesem Verzeichnis erstellt man die Datei und benennt Sie auch direkt so, dass man später erkennt, was genau hier Konfiguriert werden soll: `opsitemplates.cfg`. Nun kann man diese Datei noch mit Inhalt füllen. Zunächst die Opsi-Server:

[source,ini]
----

----

die Standardmäßig geplant sind. Es gibt ein paar Templates, die dennoch erstellt werden sollten, diese werden hier kurz erläutert und sollen einen Einblick dazu geben, zur Besseren Verständlichkeit wurden die Kommentare zu den einzelnen Einstellungen nicht gelöscht.

Die folgenden Einstellungen sind in der Regel in der Datei /etc/nagios3/templates.cfg zu erledigen.

Zunächst sollte ein neues Template-Objekt für die Opsi-Clients erstellt werden:

[source, prompt]
----
#OPSI-Clients
define host{
        name                            opsiclient    ; The name of this host template
        notifications_enabled           1       ; Host notifications are enabled
        event_handler_enabled           1       ; Host event handler is enabled
        flap_detection_enabled          1       ; Flap detection is enabled
        failure_prediction_enabled      1       ; Failure prediction is enabled
        process_perf_data               0       ; Process performance data
        retain_status_information       1       ; Retain status information across program restarts
        retain_nonstatus_information    1       ; Retain non-status information across program restarts
                max_check_attempts              10
                notification_interval           0
                notification_period             24x7
                notification_options            d,u,r
                contact_groups                  admins
        register                        0       ; DONT REGISTER THIS DEFINITION - ITS NOT A REAL HOST, JUST A TEMPLATE!
        icon_image                      opsi/opsi-client.png
        }
----

Hauptsächlich sollte dieses Template erstellt werden aus folgenden Gründen:

* Da die Clients in der Regel nicht durchlaufen, sollte die Option: "check command	check-host-alive" nicht gesetzt wird. Somit werden die Clients als Pending angezeigt und nicht als Offline.
* Optional kann durch die Option icon_image ein Image gesetzt werden, dieser muss relativ zum Pfad: /usr/share/nagios3/htdocs/images/logos/ angegeben werden.
* Optional kann auch eine eigene contact_group angegeben werden, die allerdings als Contact-Object in der contacts.cfg angelegt sein muss.

Nach dem selben Prinzip kann auch ein Template für opsi-Server erstellt werden. Auf ein weiteres Template-Beispiel wird verzichtet, da die Templates sich sehr ähneln.

Wenn pnp4nagios für die grafische Darstellung von den Performancedaten für den Opsi-Webservice eingesetzt werden, sollten noch folgende zwei Objekte als Template angleget werden:

[source,prompt]
----
define host {
   name       host-pnp
   action_url /pnp4nagios/index.php/graph?host=$HOSTNAME$&srv=_HOST_
   register   0
}

define service {
   name       srv-pnp
   action_url /pnp4nagios/index.php/graph?host=$HOSTNAME$&srv=$SERVICEDESC$
   register   0
}
----

[[opsi-Nagios-Connector-configuration-hostobjects]]
=== opsi-Server und opsi-Clients

Um die Übersichtlichkeit zu wahren, sollte unterhalb von `/etc/nagios3/conf.d` ein Verzeichnis opsi erstellt werden. Somit bleiben die opsi-Infrastruktur betreffenden Konfigurationen an einer Stelle gekapselt.

Als nächstes sollten folgende Hostgruppen erstellt werden. Dies dient zum einen der besseren Übersicht auf der Weboberfläche und hiltf beim konfigurieren der Services. Dafür sollte eine Datei Namens opsihostgroups.cfg mit folgendem Inhalt erstellt werden:

[source,prompt]
----
define hostgroup {
        hostgroup_name  opsi-clients
        alias           OPSI-Clients
}

define hostgroup {
        hostgroup_name  opsi-server
        alias           OPSI-Server
}
----

Als nächstes sollten die opsi-Server konfiguriert werden. Dies kann jeweils in einer eigenen Datei wie zum Beispiel configserver.domain.local.cfg oder eine Datei mit allen opsi-Host wie opsihost.cfg. Der Inhalt sollte für opsiserver folgendermaßen aussehen:

[source,prompt]
----
define host{
        use				opsiserver
        host_name		configserver
        hostgroups		opsi-server
        alias				opsi Configserver
        address			configserver.domain.local
        }

define host{
        use				opsiserver
        host_name		depotserver
        hostgroups		opsi-server
        alias				opsi Depotserver
        address			depotserver.domain.local
        }
----

Folgende Erläuterungen zu den Werte:
* use bezeichnet welches Template benutzt wird.
* hostgroups gibt an, welcher Hostgruppe der Server angehört.

Die opsi-Clients sollten mindestens folgendermaßen definiert werden:

[source,prompt]
----
define host{
        use				opsiclient
        host_name		client.domain.local
        hostgroups		opsi-clients
        alias				opsi client
        address			client.domain.local
        _depot_id			configserver.domain.local
        }
----

Die Clientkonfiguration bedient sich einer Sonderfunktion von Nagios. Die Option `_depot_id`
ist eine sogenannte Benutzerdefinierte Variable, die als Makro `$_depot_id$` benutzt werden kann. Dies ist nötig da die meisten Checks über den Configserver ausgeführt werden.

[[opsi-Nagios-Connector-configuration-commands]]
=== opsi Check-Kommandos Konfiguration

Die oben beschriebenen Check-Kommandos müssen nun mit den bis bisherigen Kommandos konfiguriert werden. Dafür solle eine Datei mit dem Namen opsicheckcommands.cfg erstellt werden. Hier wird eine Bespielkonfiguration angegeben, je nach dem welche Funktionen Sie auf welche Weise verwenden wollen, müssen Sie diese Einstellungen nach eigenem Ermessen modizieren.

Als erstes wird der Aufbau Anhand eines Beispiels erläutert:

[source,prompt]
----
define command{
        command_name	check_opsi_clientstatus
        command_line		$USER1$/check_opsi.py -H $_HOSTDEPOT_ID$ -u $USER2$ -p $USER3$ -P 4447 -t checkClientStatus -c $HOSTADDRESS$
        }
----

Der `command_name` dient zur Referenzierung der weiteren Konfiguration. In der Option `command_line` werden die Konfigurationen  und das Check-Kommando als erstes vereint.

Nach diesem Prinzip baut man nun die ganze Datei auf:

[source,prompt]
----
define command{
        command_name    check_opsi_clientstatus
        command_line    $USER1$/check_opsi.py -H $_HOSTDEPOT_ID$ -p 4447 -t checkClientStatus -c $HOSTADDRESS$
        }
define command{
        command_name    check_opsi_productStatus
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -P $ARG1$ -t checkProductStatus
        }
define command{
        command_name    check_opsi_productStatus_detailed
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -P $ARG1$ -t checkProductStatus -v
        }
define command{
        command_name    check_opsi_productStatus_group
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -g $ARG1$ -t checkProductStatus -v -d $HOSTADDRESS$
        }
define command{
        command_name    check_opsi_pluginonclient
        command_line    $USER1$/check_opsi.py -H $_HOSTDEPOT_ID$ -p 4447 -t checkPluginOnClient -c $HOSTADDRESS$ --plugin $ARG1$
}
define command{
        command_name    check_opsi_pluginonclient_withState
        command_line    $USER1$/check_opsi.py -H $_HOSTDEPOT_ID$ -p 4447 -t checkPluginOnClient -c $HOSTADDRESS$ --plugin $ARG1$ -s $RG2$ -o $ARG3$
}

define command {
        command_name    check_opsiwebservice
        command_line    $USER1$/check_opsi_webservice.py -H $HOSTADDRESS
}

define command {
        command_name    check_opsi_depotsync
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -t checkDepotSyncStatus -d $ARG1$
}

define command {
        command_name    check_opsi_depotsync_withexclude
        command_line    $USER1$/check_opsi.py -H $HOSTADDRESS$ -p 4447 -t checkDepotSyncStatus -d $ARG1$ -x $ARG2$
}
----


